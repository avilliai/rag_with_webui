import hashlib
import re
from collections import defaultdict
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import chromadb
from sentence_transformers import SentenceTransformer


class RAGConfig:
    """RAG ç³»ç»Ÿé…ç½®ç±»"""

    def __init__(
            self,
            # æ£€ç´¢ç›¸å…³é…ç½®
            max_results: int = 7,
            similarity_threshold: float = 0.5,
            use_hybrid_search: bool = True,
            keyword_boost: float = 0.35,

            # æ–‡æ¡£å¤„ç†é…ç½®
            chunk_size: int = 1000,
            chunk_overlap: int = 200,

            # ä¸Šä¸‹æ–‡ä¼˜åŒ–é…ç½®
            context_window_size: int = 3,
    ):
        self.max_results = max_results
        self.similarity_threshold = similarity_threshold
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.use_hybrid_search = use_hybrid_search
        self.keyword_boost = keyword_boost
        self.context_window_size = context_window_size


class RAGRetriever:
    """
    RAG æ£€ç´¢æ ¸å¿ƒæ¨¡å— - åªè´Ÿè´£æ–‡æ¡£å­˜å‚¨ã€ç´¢å¼•å’Œæ£€ç´¢
    ä¸æ¶‰åŠä»»ä½•AIå¯¹è¯åŠŸèƒ½
    """

    def __init__(
            self,
            collection_name: str = "documents_optimized",
            persist_directory: str = "./chroma_db_optimized",
            config: Optional[RAGConfig] = None
    ):
        """åˆå§‹åŒ– RAG æ£€ç´¢å™¨"""
        self.persist_directory = persist_directory
        self.config = config or RAGConfig()

        print(f"ğŸ“¦ åˆå§‹åŒ– ChromaDB (å­˜å‚¨è·¯å¾„: {persist_directory})...")
        self.client = chromadb.PersistentClient(path=persist_directory)

        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            metadata={"hnsw:space": "cosine"}
        )

        existing_count = self.collection.count()
        if existing_count > 0:
            print(f"âœ… å‘ç°å·²å­˜åœ¨çš„æ•°æ®åº“,åŒ…å« {existing_count} ä¸ªæ–‡æ¡£å—")

        print("ğŸ“¦ åŠ è½½åµŒå…¥æ¨¡å‹ (paraphrase-multilingual-MiniLM-L12-v2)...")
        self.embed_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        print("âœ… åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆ")

        print(f"\nâš™ï¸  RAG æ£€ç´¢å™¨é…ç½®:")
        print(f"   â”œâ”€ æ£€ç´¢ç­–ç•¥: {'æ··åˆæ£€ç´¢ (å…³é”®è¯+è¯­ä¹‰)' if self.config.use_hybrid_search else 'çº¯è¯­ä¹‰æ£€ç´¢'}")
        if self.config.use_hybrid_search:
            print(f"   â”‚  â””â”€ å…³é”®è¯æƒé‡: {self.config.keyword_boost}")
        print(
            f"   â”œâ”€ ä¸Šä¸‹æ–‡çª—å£æ‰©å±•: {'âœ… å·²å¯ç”¨ (çª—å£å¤§å°: ' + str(self.config.context_window_size) + ')' if self.config.context_window_size > 1 else 'âŒ æœªå¯ç”¨'}")
        print(f"   â”œâ”€ æœ€å¤§æ£€ç´¢æ•°: {self.config.max_results}")
        print(f"   â”œâ”€ ç›¸ä¼¼åº¦é˜ˆå€¼: {self.config.similarity_threshold or 'è‡ªåŠ¨'}")
        print(f"   â”œâ”€ åˆ†å—å¤§å°: {self.config.chunk_size} å­—ç¬¦")
        print(f"   â””â”€ é‡å å¤§å°: {self.config.chunk_overlap} å­—ç¬¦")

    def _get_embedding(self, text: str) -> List[float]:
        """ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡"""
        embedding = self.embed_model.encode(text, convert_to_tensor=False)
        return embedding.tolist()

    def _get_file_hash(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶çš„ MD5 å“ˆå¸Œå€¼"""
        with open(file_path, 'rb') as f:
            return hashlib.md5(f.read()).hexdigest()

    def _read_markdown_file(self, file_path: Path) -> str:
        """è¯»å– Markdown æ–‡ä»¶å†…å®¹"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            print(f"âš ï¸  è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return None

    def _extract_keywords(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯"""
        keywords = []
        hashtags = re.findall(r'#([^\s#]+)', text)
        keywords.extend(hashtags)
        bold_text = re.findall(r'\*\*([^*]+)\*\*', text)
        keywords.extend([t.strip() for t in bold_text if 3 < len(t.strip()) < 30])
        headers = re.findall(r'^#{1,6}\s+(.+)$', text, re.MULTILINE)
        keywords.extend([h.strip() for h in headers if len(h.strip()) < 50])
        names = re.findall(r'([Â·\u4e00-\u9fa5]{2,6}(?:çš„|ã€|ä¸|å’Œ|åŠ)?)', text)
        potential_names = [n.strip('çš„ã€ä¸å’ŒåŠ') for n in names if 2 <= len(n.strip('çš„ã€ä¸å’ŒåŠ')) <= 6]
        keywords.extend(potential_names)
        return list(set(keywords))

    def _extract_query_keywords(self, query: str) -> List[str]:
        """ä»æŸ¥è¯¢ä¸­æå–å…³é”®è¯"""
        keywords = set(re.findall(r'[\u4e00-\u9fa5Â·]{2,6}', query))
        key_phrases = ['æ”¿æ²»æ€æƒ³', 'ç†è®º', 'è§‚ç‚¹', 'å­¦è¯´', 'ä¸»å¼ ', 'æ‰¹åˆ¤', 'è¯„ä»·', 'æ­£ä¹‰è®º']
        for phrase in key_phrases:
            if phrase in query:
                keywords.add(phrase)
        return list(keywords)

    def _keyword_match_score(self, query: str, doc_text: str, doc_keywords: str) -> float:
        """è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°"""
        score = 0.0
        query_keywords = self._extract_query_keywords(query)
        if not query_keywords:
            return 0.0

        doc_text_lower = doc_text.lower()
        doc_keywords_lower = doc_keywords.lower() if doc_keywords else ""

        for keyword in query_keywords:
            keyword_lower = keyword.lower()
            if keyword_lower in doc_keywords_lower:
                score += 0.5
            count = min(doc_text_lower.count(keyword_lower), 5)
            score += count * 0.1
        return min(score, 1.0)

    def _parse_document_structure(self, text: str) -> List[Dict]:
        """è§£ææ–‡æ¡£ç»“æ„ï¼Œè¯†åˆ«æ ‡é¢˜å’Œç« èŠ‚"""
        lines = text.split('\n')
        sections = []
        current_section = {'content': [], 'headers': [], 'line_start': 0, 'header_level': 0}
        header_stack = []

        for i, line in enumerate(lines):
            header_match = re.match(r'^(#{1,6})\s+(.+)$', line.strip())
            if header_match:
                if current_section['content']:
                    current_section['content'] = '\n'.join(current_section['content'])
                    sections.append(current_section)

                level, title = len(header_match.group(1)), header_match.group(2).strip()
                while header_stack and header_stack[-1]['level'] >= level:
                    header_stack.pop()
                header_stack.append({'level': level, 'title': title})
                current_section = {
                    'content': [line],
                    'headers': [h['title'] for h in header_stack],
                    'header_level': level,
                    'line_start': i
                }
            else:
                current_section['content'].append(line)

        if current_section['content']:
            current_section['content'] = '\n'.join(current_section['content'])
            sections.append(current_section)
        return sections

    def _split_long_text(self, text: str, max_size: int) -> List[str]:
        """å½“å•ä¸ªç« èŠ‚å†…å®¹è¿‡é•¿æ—¶ï¼ŒæŒ‰å¥å­è¿›è¡Œåˆ†å‰²"""
        if len(text) <= max_size:
            return [text]

        sentences = re.split(r'([ã€‚ï¼ï¼Ÿ\n]+)', text)
        parts = []
        current_part = ""
        for i in range(0, len(sentences), 2):
            sentence = sentences[i]
            delimiter = sentences[i + 1] if i + 1 < len(sentences) else ""
            if len(current_part) + len(sentence) + len(delimiter) > max_size and current_part:
                parts.append(current_part)
                overlap_start = max(0, len(current_part) - self.config.chunk_overlap)
                current_part = current_part[overlap_start:] + sentence + delimiter
            else:
                current_part += sentence + delimiter
        if current_part:
            parts.append(current_part)
        return parts if parts else [text[:max_size]]

    def _smart_chunk_document(self, text: str, file_name: str) -> List[tuple]:
        """æ™ºèƒ½åˆ†å—æ–‡æ¡£ï¼šåŸºäºMarkdownç»“æ„"""
        sections = self._parse_document_structure(text)
        chunks = []
        chunk_idx = 0

        i = 0
        while i < len(sections):
            current_section = sections[i]
            headers_text = ' > '.join(current_section.get('headers', []))

            chunk_content_parts = []
            current_size = 0
            start_section_idx = i

            j = i
            while j < len(sections):
                section_to_add = sections[j]
                content_to_add = section_to_add['content']

                if j > start_section_idx and section_to_add.get('header_level', 6) <= 2:
                    break

                if current_size > 0 and current_size + len(content_to_add) > self.config.chunk_size:
                    break

                chunk_content_parts.append(content_to_add)
                current_size += len(content_to_add)
                j += 1

            full_content = "\n\n".join(chunk_content_parts)

            if len(full_content) > self.config.chunk_size:
                split_parts = self._split_long_text(full_content, self.config.chunk_size)
            else:
                split_parts = [full_content]

            for part in split_parts:
                keywords = self._extract_keywords(f"# {headers_text}\n{part}")
                chunk_meta = {
                    'section_path': headers_text,
                    'keywords': ', '.join(keywords),
                }
                chunk_id = f"{file_name}_chunk_{chunk_idx}"
                chunks.append((part, chunk_id, len(part), chunk_meta))
                chunk_idx += 1

            if j > start_section_idx + 1:
                i = j - 1
            else:
                i = j

        return chunks

    def load_documents_from_folder(self, folder_path: str = "./docs", force_reload: bool = False):
        """ä»æ–‡ä»¶å¤¹é€’å½’åŠ è½½æ‰€æœ‰ Markdown æ–‡æ¡£"""
        docs_path = Path(folder_path)
        if not docs_path.exists():
            print(f"âŒ æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
            return

        md_files = list(docs_path.rglob("*.md"))
        if not md_files:
            print(f"âš ï¸  åœ¨ {folder_path} ä¸­æ²¡æœ‰æ‰¾åˆ° Markdown æ–‡ä»¶ã€‚")
            return

        print(f"\nğŸ“ æ‰¾åˆ° {len(md_files)} ä¸ª Markdown æ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")

        total_docs_in_db = self.collection.count()
        all_metadatas = self.collection.get(limit=total_docs_in_db, include=["metadatas"])[
            'metadatas'] if total_docs_in_db > 0 else []
        existing_hashes = {meta.get('source'): meta.get('file_hash') for meta in all_metadatas if
                           'source' in meta and 'file_hash' in meta}

        new_docs_content = []
        new_docs_for_embedding = []
        new_ids = []
        new_metadatas = []
        ids_to_delete = []
        updated_count, new_count, skipped_count = 0, 0, 0

        for md_file in md_files:
            relative_path = str(md_file.relative_to(docs_path)).replace('\\', '/')
            current_hash = self._get_file_hash(str(md_file))

            if relative_path in existing_hashes:
                if not force_reload and existing_hashes[relative_path] == current_hash:
                    skipped_count += 1
                    continue
                else:
                    print(f"ğŸ”„ æ£€æµ‹åˆ°æ–‡ä»¶å˜æ›´: {relative_path}")
                    updated_count += 1
                    results = self.collection.get(where={"source": relative_path}, include=[])
                    ids_to_delete.extend(results['ids'])
            else:
                new_count += 1

            content = self._read_markdown_file(md_file)
            if content:
                chunks = self._smart_chunk_document(content, relative_path)
                for chunk_text, chunk_id, chunk_size, chunk_meta in chunks:
                    new_docs_content.append(chunk_text)
                    new_ids.append(chunk_id)

                    embedding_text = f"æ‰€å±ç« èŠ‚: {chunk_meta.get('section_path', '')}\nå…³é”®è¯: {chunk_meta.get('keywords', '')}\n\nå†…å®¹:\n{chunk_text}"
                    new_docs_for_embedding.append(embedding_text)

                    metadata = {
                        "source": relative_path,
                        "file_hash": current_hash,
                        **chunk_meta
                    }
                    new_metadatas.append(metadata)

        if ids_to_delete:
            print(f"ğŸ—‘ï¸  æ­£åœ¨åˆ é™¤ {len(ids_to_delete)} ä¸ªæ—§çš„æ–‡æ¡£å—...")
            delete_batch_size = 500
            for i in range(0, len(ids_to_delete), delete_batch_size):
                self.collection.delete(ids=ids_to_delete[i:i + delete_batch_size])
            print("âœ… æ—§æ–‡æ¡£å—åˆ é™¤å®Œæ¯•ã€‚")

        if new_docs_content:
            print(
                f"\nğŸ’¾ å‡†å¤‡å¤„ç† {new_count} ä¸ªæ–°æ–‡ä»¶å’Œ {updated_count} ä¸ªæ›´æ–°æ–‡ä»¶ï¼Œå…±è®¡ {len(new_docs_content)} ä¸ªæ–°æ–‡æ¡£å—...")
            print("ğŸ”„ ç”Ÿæˆå¢å¼ºåµŒå…¥å‘é‡ï¼ˆå¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰...")

            embedding_batch_size = 32
            embeddings = []
            for i in range(0, len(new_docs_for_embedding), embedding_batch_size):
                batch = new_docs_for_embedding[i:i + embedding_batch_size]
                batch_embeddings = self.embed_model.encode(batch, convert_to_tensor=False,
                                                           show_progress_bar=False).tolist()
                embeddings.extend(batch_embeddings)
                print(
                    f"   ç”ŸæˆåµŒå…¥å‘é‡è¿›åº¦: {min(i + embedding_batch_size, len(new_docs_for_embedding))}/{len(new_docs_for_embedding)}")

            db_batch_size = 4000
            total_batches = (len(new_ids) + db_batch_size - 1) // db_batch_size
            print(f"\nâ• æ­£åœ¨å°† {len(new_ids)} ä¸ªæ–‡æ¡£å—åˆ† {total_batches} æ‰¹æ·»åŠ åˆ°æ•°æ®åº“...")

            for i in range(0, len(new_ids), db_batch_size):
                batch_ids = new_ids[i:i + db_batch_size]
                batch_documents = new_docs_content[i:i + db_batch_size]
                batch_embeddings = embeddings[i:i + db_batch_size]
                batch_metadatas = new_metadatas[i:i + db_batch_size]

                self.collection.add(
                    ids=batch_ids,
                    documents=batch_documents,
                    embeddings=batch_embeddings,
                    metadatas=batch_metadatas
                )
                print(f"   æ‰¹æ¬¡ {i // db_batch_size + 1}/{total_batches} æ·»åŠ æˆåŠŸã€‚")

            print(f"âœ… æˆåŠŸæ·»åŠ /æ›´æ–° {len(new_docs_content)} ä¸ªæ–‡æ¡£å—ã€‚")

        if skipped_count > 0:
            print(f"â­ï¸  è·³è¿‡ {skipped_count} ä¸ªæœªä¿®æ”¹çš„æ–‡æ¡£ã€‚")

        print(f"\nğŸ“Š æ•°æ®åº“å½“å‰å…±æœ‰ {self.collection.count()} ä¸ªæ–‡æ¡£å—ã€‚")

    def search(self, query: str, n_results: Optional[int] = None) -> dict:
        """æ··åˆæ£€ç´¢"""
        query_embedding = self._get_embedding(query)
        n_results = n_results or self.config.max_results
        search_n = min(n_results * 5, self.collection.count())

        semantic_results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=search_n,
            include=["metadatas", "documents", "distances"]
        )

        if not semantic_results['documents'][0]:
            return semantic_results

        if self.config.use_hybrid_search:
            scored_results = []
            for doc_id, doc, meta, dist in zip(
                    semantic_results['ids'][0],
                    semantic_results['documents'][0],
                    semantic_results['metadatas'][0],
                    semantic_results['distances'][0]
            ):
                semantic_score = 1 - dist
                keyword_score = self._keyword_match_score(query, doc, meta.get('keywords', ''))
                final_score = semantic_score * (
                            1 - self.config.keyword_boost) + keyword_score * self.config.keyword_boost
                scored_results.append({
                    'id': doc_id,
                    'doc': doc,
                    'meta': meta,
                    'dist': dist,
                    'score': final_score
                })

            scored_results.sort(key=lambda x: x['score'], reverse=True)
            filtered = [r for r in scored_results if (1 - r['dist']) >= self.config.similarity_threshold]
            top_results = filtered[:n_results]

            return {
                'ids': [[r['id'] for r in top_results]],
                'documents': [[r['doc'] for r in top_results]],
                'metadatas': [[r['meta'] for r in top_results]],
                'distances': [[r['dist'] for r in top_results]],
            }

        indices = [i for i, d in enumerate(semantic_results['distances'][0]) if
                   (1 - d) >= self.config.similarity_threshold]
        top_indices = indices[:n_results]
        return {
            'ids': [[semantic_results['ids'][0][i] for i in top_indices]],
            'documents': [[semantic_results['documents'][0][i] for i in top_indices]],
            'metadatas': [[semantic_results['metadatas'][0][i] for i in top_indices]],
            'distances': [[semantic_results['distances'][0][i] for i in top_indices]],
        }

    def expand_context_with_window(self, search_results: dict) -> List[Dict]:
        """æ‰©å±•æ£€ç´¢ç»“æœçš„ä¸Šä¸‹æ–‡çª—å£"""
        if self.config.context_window_size <= 1 or not search_results['ids'][0]:
            return [{"doc": doc, "meta": meta, "is_hit": True} for doc, meta in
                    zip(search_results['documents'][0], search_results['metadatas'][0])]

        print("ğŸ”„ æ­£åœ¨æ‰©å±•ä¸Šä¸‹æ–‡çª—å£...")
        final_docs = {
            r_id: {"doc": doc, "meta": meta, "is_hit": True}
            for r_id, doc, meta in zip(
                search_results['ids'][0],
                search_results['documents'][0],
                search_results['metadatas'][0]
            )
        }

        ids_to_fetch = set()
        window_radius = self.config.context_window_size // 2

        for r_id in search_results['ids'][0]:
            parts = r_id.rsplit('_chunk_', 1)
            if len(parts) == 2 and parts[1].isdigit():
                base_name, index = parts[0], int(parts[1])
                for i in range(1, window_radius + 1):
                    prev_id = f"{base_name}_chunk_{index - i}"
                    next_id = f"{base_name}_chunk_{index + i}"
                    if prev_id not in final_docs:
                        ids_to_fetch.add(prev_id)
                    if next_id not in final_docs:
                        ids_to_fetch.add(next_id)

        if ids_to_fetch:
            ids_list = list(ids_to_fetch)
            get_batch_size = 500
            for i in range(0, len(ids_list), get_batch_size):
                batch_ids = ids_list[i:i + get_batch_size]
                context_docs = self.collection.get(ids=batch_ids, include=["metadatas", "documents"])
                for c_id, doc, meta in zip(context_docs['ids'], context_docs['documents'], context_docs['metadatas']):
                    if c_id not in final_docs:
                        final_docs[c_id] = {"doc": doc, "meta": meta, "is_hit": False}

        sorted_ids = sorted(
            final_docs.keys(),
            key=lambda x: (final_docs[x]['meta']['source'], int(x.rsplit('_', 1)[1]))
        )
        return [final_docs[id] for id in sorted_ids]

    def get_stats(self) -> dict:
        """è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        count = self.collection.count()
        results = self.collection.get()
        docs_by_source = defaultdict(list)

        for metadata in results['metadatas']:
            source = metadata.get('source', 'unknown')
            docs_by_source[source].append(metadata)

        doc_list = []
        for source, metadatas in sorted(docs_by_source.items()):
            doc_list.append({
                'source': source,
                'chunk_count': len(metadatas),
            })

        return {
            'total_chunks': count,
            'total_documents': len(doc_list),
            'documents': doc_list
        }