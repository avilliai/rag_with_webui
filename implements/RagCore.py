import hashlib
import re
from collections import defaultdict
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import chromadb
from sentence_transformers import SentenceTransformer

# PDFå’ŒEPUBæ”¯æŒ
try:
    import PyPDF2
    import fitz  # PyMuPDF

    PDF_SUPPORT = True
except ImportError:
    PDF_SUPPORT = False
    print("âš ï¸  æœªå®‰è£…PDFåº“,è¯·è¿è¡Œ: pip install PyPDF2 PyMuPDF")

try:
    import ebooklib
    from ebooklib import epub
    from bs4 import BeautifulSoup

    EPUB_SUPPORT = True
except ImportError:
    EPUB_SUPPORT = False
    print("âš ï¸  æœªå®‰è£…EPUBåº“,è¯·è¿è¡Œ: pip install ebooklib beautifulsoup4")


class RAGConfig:
    """RAG ç³»ç»Ÿé…ç½®ç±»"""

    def __init__(
            self,
            # æ£€ç´¢ç›¸å…³é…ç½®
            max_results: int = 7,
            similarity_threshold: float = 0.5,
            use_hybrid_search: bool = True,
            keyword_boost: float = 0.35,

            # æ–‡æ¡£å¤„ç†é…ç½®
            chunk_size: int = 1000,
            chunk_overlap: int = 200,

            # ä¸Šä¸‹æ–‡ä¼˜åŒ–é…ç½®
            context_window_size: int = 3,
    ):
        self.max_results = max_results
        self.similarity_threshold = similarity_threshold
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.use_hybrid_search = use_hybrid_search
        self.keyword_boost = keyword_boost
        self.context_window_size = context_window_size


class RAGRetriever:
    """
    RAG æ£€ç´¢æ ¸å¿ƒæ¨¡å— - åªè´Ÿè´£æ–‡æ¡£å­˜å‚¨ã€ç´¢å¼•å’Œæ£€ç´¢
    ä¸æ¶‰åŠä»»ä½•AIå¯¹è¯åŠŸèƒ½
    """

    def __init__(
            self,
            collection_name: str = "documents_optimized",
            persist_directory: str = "./chroma_db_optimized",
            config: Optional[RAGConfig] = None
    ):
        """åˆå§‹åŒ– RAG æ£€ç´¢å™¨"""
        self.persist_directory = persist_directory
        self.config = config or RAGConfig()

        print(f"ğŸ“¦ åˆå§‹åŒ– ChromaDB (å­˜å‚¨è·¯å¾„: {persist_directory})...")
        self.client = chromadb.PersistentClient(path=persist_directory)

        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            metadata={"hnsw:space": "cosine"}
        )

        existing_count = self.collection.count()
        if existing_count > 0:
            print(f"âœ… å‘ç°å·²å­˜åœ¨çš„æ•°æ®åº“,åŒ…å« {existing_count} ä¸ªæ–‡æ¡£å—")

        print("ğŸ“¦ åŠ è½½åµŒå…¥æ¨¡å‹ (paraphrase-multilingual-MiniLM-L12-v2)...")
        self.embed_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        print("âœ… åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆ")

        print(f"\nâš™ï¸  RAG æ£€ç´¢å™¨é…ç½®:")
        print(f"   â”œâ”€ æ£€ç´¢ç­–ç•¥: {'æ··åˆæ£€ç´¢ (å…³é”®è¯+è¯­ä¹‰)' if self.config.use_hybrid_search else 'çº¯è¯­ä¹‰æ£€ç´¢'}")
        if self.config.use_hybrid_search:
            print(f"   â”‚  â””â”€ å…³é”®è¯æƒé‡: {self.config.keyword_boost}")
        print(
            f"   â”œâ”€ ä¸Šä¸‹æ–‡çª—å£æ‰©å±•: {'âœ… å·²å¯ç”¨ (çª—å£å¤§å°: ' + str(self.config.context_window_size) + ')' if self.config.context_window_size > 1 else 'âŒ æœªå¯ç”¨'}")
        print(f"   â”œâ”€ æœ€å¤§æ£€ç´¢æ•°: {self.config.max_results}")
        print(f"   â”œâ”€ ç›¸ä¼¼åº¦é˜ˆå€¼: {self.config.similarity_threshold or 'è‡ªåŠ¨'}")
        print(f"   â”œâ”€ åˆ†å—å¤§å°: {self.config.chunk_size} å­—ç¬¦")
        print(f"   â””â”€ é‡å å¤§å°: {self.config.chunk_overlap} å­—ç¬¦")

    def _get_embedding(self, text: str) -> List[float]:
        """ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡"""
        embedding = self.embed_model.encode(text, convert_to_tensor=False)
        return embedding.tolist()

    def _get_file_hash(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶çš„ MD5 å“ˆå¸Œå€¼"""
        with open(file_path, 'rb') as f:
            return hashlib.md5(f.read()).hexdigest()

    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬ä¸­çš„å¼‚å¸¸å­—ç¬¦å’Œæ ¼å¼é—®é¢˜"""
        if not text:
            return ""

        # ç§»é™¤ç©ºå­—ç¬¦å’Œæ§åˆ¶å­—ç¬¦ï¼ˆä¿ç•™æ¢è¡Œç¬¦ã€åˆ¶è¡¨ç¬¦ï¼‰
        text = ''.join(char for char in text if char.isprintable() or char in '\n\t')

        # è§„èŒƒåŒ–ç©ºç™½å­—ç¬¦
        text = re.sub(r'[ \t]+', ' ', text)  # å¤šä¸ªç©ºæ ¼/åˆ¶è¡¨ç¬¦å˜ä¸ºä¸€ä¸ªç©ºæ ¼
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)  # å¤šä¸ªç©ºè¡Œå˜ä¸ºä¸¤ä¸ªæ¢è¡Œ

        # ç§»é™¤é¡µçœ‰é¡µè„šå¸¸è§æ¨¡å¼
        text = re.sub(r'ç¬¬\s*\d+\s*é¡µ.*?å…±\s*\d+\s*é¡µ', '', text)
        text = re.sub(r'Page\s+\d+\s+of\s+\d+', '', text, flags=re.IGNORECASE)

        return text.strip()

    def _read_pdf_file(self, file_path: Path) -> str:
        """è¯»å–PDFæ–‡ä»¶,ä½¿ç”¨å¤šç§æ–¹æ³•ç¡®ä¿å¥å£®æ€§"""
        if not PDF_SUPPORT:
            print(f"âš ï¸  è·³è¿‡PDFæ–‡ä»¶ {file_path}: æœªå®‰è£…PDFæ”¯æŒåº“")
            return None

        text = ""

        # æ–¹æ³•1: ä½¿ç”¨PyMuPDF (fitz) - å¯¹ä¸­æ–‡æ”¯æŒæ›´å¥½
        try:
            doc = fitz.open(str(file_path))
            for page_num, page in enumerate(doc, 1):
                try:
                    page_text = page.get_text()
                    if page_text.strip():
                        text += f"\n\n--- ç¬¬ {page_num} é¡µ ---\n\n{page_text}"
                except Exception as e:
                    print(f"   âš ï¸  é¡µé¢ {page_num} æå–å¤±è´¥: {e}")
                    continue
            doc.close()

            if text.strip():
                return self._clean_text(text)
        except Exception as e:
            print(f"   âš ï¸  PyMuPDFæå–å¤±è´¥: {e}, å°è¯•å¤‡ç”¨æ–¹æ³•...")

        # æ–¹æ³•2: ä½¿ç”¨PyPDF2ä½œä¸ºå¤‡ç”¨
        try:
            with open(file_path, 'rb') as f:
                pdf_reader = PyPDF2.PdfReader(f)
                for page_num in range(len(pdf_reader.pages)):
                    try:
                        page = pdf_reader.pages[page_num]
                        page_text = page.extract_text()
                        if page_text.strip():
                            text += f"\n\n--- ç¬¬ {page_num + 1} é¡µ ---\n\n{page_text}"
                    except Exception as e:
                        print(f"   âš ï¸  é¡µé¢ {page_num + 1} æå–å¤±è´¥: {e}")
                        continue

            if text.strip():
                return self._clean_text(text)
        except Exception as e:
            print(f"   âš ï¸  PyPDF2æå–å¤±è´¥: {e}")

        # å¦‚æœä¸¤ç§æ–¹æ³•éƒ½å¤±è´¥
        if not text.strip():
            print(f"   âŒ æ— æ³•ä»PDFæå–æ–‡æœ¬: {file_path}")
            return None

        return self._clean_text(text)

    def _read_epub_file(self, file_path: Path) -> str:
        """è¯»å–EPUBæ–‡ä»¶"""
        if not EPUB_SUPPORT:
            print(f"âš ï¸  è·³è¿‡EPUBæ–‡ä»¶ {file_path}: æœªå®‰è£…EPUBæ”¯æŒåº“")
            return None

        try:
            book = epub.read_epub(str(file_path))
            text_parts = []

            for item in book.get_items():
                if item.get_type() == ebooklib.ITEM_DOCUMENT:
                    try:
                        content = item.get_content().decode('utf-8', errors='ignore')
                        soup = BeautifulSoup(content, 'html.parser')

                        # ç§»é™¤scriptå’Œstyleæ ‡ç­¾
                        for script in soup(["script", "style"]):
                            script.decompose()

                        # æå–æ–‡æœ¬
                        text = soup.get_text(separator='\n', strip=True)
                        if text.strip():
                            text_parts.append(text)
                    except Exception as e:
                        print(f"   âš ï¸  EPUBç« èŠ‚è§£æå¤±è´¥: {e}")
                        continue

            if not text_parts:
                print(f"   âŒ EPUBæ–‡ä»¶ä¸ºç©º: {file_path}")
                return None

            full_text = "\n\n".join(text_parts)
            return self._clean_text(full_text)

        except Exception as e:
            print(f"   âŒ è¯»å–EPUBå¤±è´¥ {file_path}: {e}")
            return None

    def _read_markdown_file(self, file_path: Path) -> str:
        """è¯»å– Markdown æ–‡ä»¶å†…å®¹"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            print(f"âš ï¸  è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return None

    def _read_file(self, file_path: Path) -> str:
        """æ ¹æ®æ–‡ä»¶ç±»å‹è¯»å–æ–‡ä»¶å†…å®¹"""
        suffix = file_path.suffix.lower()

        if suffix == '.md':
            return self._read_markdown_file(file_path)
        elif suffix == '.pdf':
            return self._read_pdf_file(file_path)
        elif suffix == '.epub':
            return self._read_epub_file(file_path)
        else:
            print(f"âš ï¸  ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_path}")
            return None

    def _extract_keywords(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯"""
        keywords = []
        hashtags = re.findall(r'#([^\s#]+)', text)
        keywords.extend(hashtags)
        bold_text = re.findall(r'\*\*([^*]+)\*\*', text)
        keywords.extend([t.strip() for t in bold_text if 3 < len(t.strip()) < 30])
        headers = re.findall(r'^#{1,6}\s+(.+)$', text, re.MULTILINE)
        keywords.extend([h.strip() for h in headers if len(h.strip()) < 50])
        names = re.findall(r'([Â·\u4e00-\u9fa5]{2,6}(?:çš„|ã€|ä¸|å’Œ|åŠ)?)', text)
        potential_names = [n.strip('çš„ã€ä¸å’ŒåŠ') for n in names if 2 <= len(n.strip('çš„ã€ä¸å’ŒåŠ')) <= 6]
        keywords.extend(potential_names)
        return list(set(keywords))

    def _extract_query_keywords(self, query: str) -> List[str]:
        """ä»æŸ¥è¯¢ä¸­æå–å…³é”®è¯"""
        keywords = set(re.findall(r'[\u4e00-\u9fa5Â·]{2,6}', query))
        key_phrases = ['æ”¿æ²»æ€æƒ³', 'ç†è®º', 'è§‚ç‚¹', 'å­¦è¯´', 'ä¸»å¼ ', 'æ‰¹åˆ¤', 'è¯„ä»·', 'æ­£ä¹‰è®º']
        for phrase in key_phrases:
            if phrase in query:
                keywords.add(phrase)
        return list(keywords)

    def _keyword_match_score(self, query: str, doc_text: str, doc_keywords: str) -> float:
        """è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°"""
        score = 0.0
        query_keywords = self._extract_query_keywords(query)
        if not query_keywords:
            return 0.0

        doc_text_lower = doc_text.lower()
        doc_keywords_lower = doc_keywords.lower() if doc_keywords else ""

        for keyword in query_keywords:
            keyword_lower = keyword.lower()
            if keyword_lower in doc_keywords_lower:
                score += 0.5
            count = min(doc_text_lower.count(keyword_lower), 5)
            score += count * 0.1
        return min(score, 1.0)

    def _parse_document_structure(self, text: str) -> List[Dict]:
        """è§£ææ–‡æ¡£ç»“æ„ï¼Œè¯†åˆ«æ ‡é¢˜å’Œç« èŠ‚"""
        lines = text.split('\n')
        sections = []
        current_section = {'content': [], 'headers': [], 'line_start': 0, 'header_level': 0}
        header_stack = []

        for i, line in enumerate(lines):
            header_match = re.match(r'^(#{1,6})\s+(.+)$', line.strip())
            if header_match:
                if current_section['content']:
                    current_section['content'] = '\n'.join(current_section['content'])
                    sections.append(current_section)

                level, title = len(header_match.group(1)), header_match.group(2).strip()
                while header_stack and header_stack[-1]['level'] >= level:
                    header_stack.pop()
                header_stack.append({'level': level, 'title': title})
                current_section = {
                    'content': [line],
                    'headers': [h['title'] for h in header_stack],
                    'header_level': level,
                    'line_start': i
                }
            else:
                current_section['content'].append(line)

        if current_section['content']:
            current_section['content'] = '\n'.join(current_section['content'])
            sections.append(current_section)
        return sections

    def _split_long_text(self, text: str, max_size: int) -> List[str]:
        """å½“å•ä¸ªç« èŠ‚å†…å®¹è¿‡é•¿æ—¶ï¼ŒæŒ‰å¥å­è¿›è¡Œåˆ†å‰²"""
        if len(text) <= max_size:
            return [text]

        sentences = re.split(r'([ã€‚ï¼ï¼Ÿ\n]+)', text)
        parts = []
        current_part = ""
        for i in range(0, len(sentences), 2):
            sentence = sentences[i]
            delimiter = sentences[i + 1] if i + 1 < len(sentences) else ""
            if len(current_part) + len(sentence) + len(delimiter) > max_size and current_part:
                parts.append(current_part)
                overlap_start = max(0, len(current_part) - self.config.chunk_overlap)
                current_part = current_part[overlap_start:] + sentence + delimiter
            else:
                current_part += sentence + delimiter
        if current_part:
            parts.append(current_part)
        return parts if parts else [text[:max_size]]

    def _smart_chunk_document(self, text: str, file_name: str) -> List[tuple]:
        """æ™ºèƒ½åˆ†å—æ–‡æ¡£ï¼šåŸºäºMarkdownç»“æ„"""
        sections = self._parse_document_structure(text)
        chunks = []
        chunk_idx = 0

        i = 0
        while i < len(sections):
            current_section = sections[i]
            headers_text = ' > '.join(current_section.get('headers', []))

            chunk_content_parts = []
            current_size = 0
            start_section_idx = i

            j = i
            while j < len(sections):
                section_to_add = sections[j]
                content_to_add = section_to_add['content']

                if j > start_section_idx and section_to_add.get('header_level', 6) <= 2:
                    break

                if current_size > 0 and current_size + len(content_to_add) > self.config.chunk_size:
                    break

                chunk_content_parts.append(content_to_add)
                current_size += len(content_to_add)
                j += 1

            full_content = "\n\n".join(chunk_content_parts)

            if len(full_content) > self.config.chunk_size:
                split_parts = self._split_long_text(full_content, self.config.chunk_size)
            else:
                split_parts = [full_content]

            for part in split_parts:
                keywords = self._extract_keywords(f"# {headers_text}\n{part}")
                chunk_meta = {
                    'section_path': headers_text,
                    'keywords': ', '.join(keywords),
                }
                chunk_id = f"{file_name}_chunk_{chunk_idx}"
                chunks.append((part, chunk_id, len(part), chunk_meta))
                chunk_idx += 1

            if j > start_section_idx + 1:
                i = j - 1
            else:
                i = j

        return chunks

    def load_documents_from_folder(self, folder_path: str = "./docs", force_reload: bool = False):
        """ä»æ–‡ä»¶å¤¹é€’å½’åŠ è½½æ‰€æœ‰æ”¯æŒçš„æ–‡æ¡£ - æ™ºèƒ½å¢é‡æ›´æ–°ç‰ˆæœ¬"""
        docs_path = Path(folder_path)
        if not docs_path.exists():
            print(f"âŒ æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
            return

        # æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å
        supported_extensions = ['.md', '.pdf', '.epub']
        all_files = []
        for ext in supported_extensions:
            all_files.extend(docs_path.rglob(f"*{ext}"))

        if not all_files:
            print(f"âš ï¸  åœ¨ {folder_path} ä¸­æ²¡æœ‰æ‰¾åˆ°æ”¯æŒçš„æ–‡æ¡£æ–‡ä»¶ã€‚")
            return

        print(f"\nğŸ“ æ‰¾åˆ° {len(all_files)} ä¸ªæ–‡æ¡£æ–‡ä»¶ï¼Œå¼€å§‹æ™ºèƒ½å¢é‡åˆ†æ...")
        print(f"   æ”¯æŒæ ¼å¼: {', '.join(supported_extensions)}")

        # ========== ç¬¬ä¸€æ­¥: è·å–æ•°æ®åº“ç°æœ‰æ–‡ä»¶çŠ¶æ€ ==========
        total_docs_in_db = self.collection.count()
        all_metadatas = self.collection.get(limit=total_docs_in_db, include=["metadatas"])[
            'metadatas'] if total_docs_in_db > 0 else []

        # æ„å»ºæ•°æ®åº“ä¸­çš„æ–‡ä»¶æ˜ å°„: {ç›¸å¯¹è·¯å¾„: å“ˆå¸Œå€¼}
        existing_files_in_db = {}
        for meta in all_metadatas:
            if 'source' in meta and 'file_hash' in meta:
                existing_files_in_db[meta['source']] = meta['file_hash']

        print(f"   æ•°æ®åº“ä¸­ç°æœ‰ {len(existing_files_in_db)} ä¸ªæ–‡æ¡£è®°å½•")

        # ========== ç¬¬äºŒæ­¥: æ‰«ææ–‡ä»¶ç³»ç»Ÿ,æ„å»ºå½“å‰æ–‡ä»¶çŠ¶æ€ ==========
        current_files = {}  # {ç›¸å¯¹è·¯å¾„: (å®Œæ•´è·¯å¾„, å“ˆå¸Œå€¼)}
        for doc_file in all_files:
            relative_path = str(doc_file.relative_to(docs_path)).replace('\\', '/')
            current_hash = self._get_file_hash(str(doc_file))
            current_files[relative_path] = (doc_file, current_hash)

        # ========== ç¬¬ä¸‰æ­¥: åˆ†ç±»æ–‡ä»¶çŠ¶æ€ ==========
        files_to_add = []  # æ–°å¢æ–‡ä»¶
        files_to_update = []  # ä¿®æ”¹æ–‡ä»¶
        files_to_delete = []  # åˆ é™¤æ–‡ä»¶
        files_unchanged = []  # æœªå˜åŒ–æ–‡ä»¶

        # æ£€æµ‹æ–°å¢å’Œä¿®æ”¹
        for relative_path, (doc_file, current_hash) in current_files.items():
            if relative_path not in existing_files_in_db:
                files_to_add.append((relative_path, doc_file, current_hash))
            elif existing_files_in_db[relative_path] != current_hash:
                files_to_update.append((relative_path, doc_file, current_hash))
            else:
                files_unchanged.append(relative_path)

        # æ£€æµ‹åˆ é™¤ (æ•°æ®åº“ä¸­æœ‰,ä½†æ–‡ä»¶ç³»ç»Ÿä¸­æ²¡æœ‰)
        for db_path in existing_files_in_db.keys():
            if db_path not in current_files:
                files_to_delete.append(db_path)

        # ========== ç¬¬å››æ­¥: è¾“å‡ºå˜æ›´æ‘˜è¦ ==========
        print(f"\nğŸ“Š æ–‡ä»¶å˜æ›´åˆ†æ:")
        print(f"   âœ… æœªå˜åŒ–: {len(files_unchanged)} ä¸ª")
        print(f"   â• æ–°å¢:   {len(files_to_add)} ä¸ª")
        print(f"   ğŸ”„ ä¿®æ”¹:   {len(files_to_update)} ä¸ª")
        print(f"   ğŸ—‘ï¸  åˆ é™¤:   {len(files_to_delete)} ä¸ª")

        # å¦‚æœæ²¡æœ‰ä»»ä½•å˜æ›´ä¸”ä¸å¼ºåˆ¶é‡è½½,ç›´æ¥è¿”å›
        if not force_reload and not files_to_add and not files_to_update and not files_to_delete:
            print(f"\nâœ… æ‰€æœ‰æ–‡æ¡£éƒ½æ˜¯æœ€æ–°çš„ï¼Œæ— éœ€å¤„ç†ã€‚")
            print(f"ğŸ“Š æ•°æ®åº“å½“å‰å…±æœ‰ {self.collection.count()} ä¸ªæ–‡æ¡£å—ã€‚")
            return

        # ========== ç¬¬äº”æ­¥: å¤„ç†åˆ é™¤çš„æ–‡ä»¶ ==========
        if files_to_delete:
            print(f"\nğŸ—‘ï¸  æ­£åœ¨åˆ é™¤ {len(files_to_delete)} ä¸ªå·²åˆ é™¤æ–‡æ¡£...")
            ids_to_delete = []
            for del_path in files_to_delete:
                print(f"   ğŸ—‘ï¸  åˆ é™¤: {del_path}")
                results = self.collection.get(where={"source": del_path}, include=[])
                ids_to_delete.extend(results['ids'])

            if ids_to_delete:
                delete_batch_size = 500
                for i in range(0, len(ids_to_delete), delete_batch_size):
                    self.collection.delete(ids=ids_to_delete[i:i + delete_batch_size])
                print(f"âœ… å·²åˆ é™¤ {len(ids_to_delete)} ä¸ªæ–‡æ¡£å—")

        # ========== ç¬¬å…­æ­¥: å¤„ç†æ›´æ–°çš„æ–‡ä»¶ ==========
        if files_to_update:
            print(f"\nğŸ”„ æ­£åœ¨å¤„ç† {len(files_to_update)} ä¸ªä¿®æ”¹çš„æ–‡æ¡£...")
            ids_to_delete = []
            for relative_path, doc_file, current_hash in files_to_update:
                print(f"   ğŸ”„ æ›´æ–°: {relative_path}")
                results = self.collection.get(where={"source": relative_path}, include=[])
                ids_to_delete.extend(results['ids'])

            if ids_to_delete:
                delete_batch_size = 500
                for i in range(0, len(ids_to_delete), delete_batch_size):
                    self.collection.delete(ids=ids_to_delete[i:i + delete_batch_size])
                print(f"âœ… å·²åˆ é™¤ {len(ids_to_delete)} ä¸ªæ—§æ–‡æ¡£å—")

        # ========== ç¬¬ä¸ƒæ­¥: æ·»åŠ æ–°æ–‡æ¡£å’Œæ›´æ–°æ–‡æ¡£ ==========
        files_to_process = files_to_add + files_to_update

        if not files_to_process:
            print(f"\nğŸ“Š æ•°æ®åº“å½“å‰å…±æœ‰ {self.collection.count()} ä¸ªæ–‡æ¡£å—ã€‚")
            return

        print(f"\nğŸ’¾ å¼€å§‹å¤„ç† {len(files_to_process)} ä¸ªæ–‡æ¡£...")

        new_docs_content = []
        new_docs_for_embedding = []
        new_ids = []
        new_metadatas = []

        for relative_path, doc_file, current_hash in files_to_process:
            action = "æ–°å¢" if (relative_path, doc_file, current_hash) in files_to_add else "æ›´æ–°"
            print(f"\nğŸ“„ {action}: {relative_path}")

            content = self._read_file(doc_file)
            if content:
                chunks = self._smart_chunk_document(content, relative_path)
                print(f"   âœ… ç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æ¡£å—")

                for chunk_text, chunk_id, chunk_size, chunk_meta in chunks:
                    new_docs_content.append(chunk_text)
                    new_ids.append(chunk_id)

                    embedding_text = f"æ‰€å±ç« èŠ‚: {chunk_meta.get('section_path', '')}\nå…³é”®è¯: {chunk_meta.get('keywords', '')}\n\nå†…å®¹:\n{chunk_text}"
                    new_docs_for_embedding.append(embedding_text)

                    metadata = {
                        "source": relative_path,
                        "file_hash": current_hash,
                        **chunk_meta
                    }
                    new_metadatas.append(metadata)
            else:
                print(f"   âŒ æ–‡ä»¶è¯»å–å¤±è´¥ï¼Œè·³è¿‡")

        # ========== ç¬¬å…«æ­¥: ç”ŸæˆåµŒå…¥å¹¶æ·»åŠ åˆ°æ•°æ®åº“ ==========
        if new_docs_content:
            print(f"\nğŸ’¾ å…±è®¡ {len(new_docs_content)} ä¸ªæ–°æ–‡æ¡£å—å¾…å¤„ç†...")
            print("ğŸ”„ ç”ŸæˆåµŒå…¥å‘é‡ï¼ˆå¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰...")

            embedding_batch_size = 32
            embeddings = []
            for i in range(0, len(new_docs_for_embedding), embedding_batch_size):
                batch = new_docs_for_embedding[i:i + embedding_batch_size]
                batch_embeddings = self.embed_model.encode(batch, convert_to_tensor=False,
                                                           show_progress_bar=False).tolist()
                embeddings.extend(batch_embeddings)
                print(
                    f"   åµŒå…¥å‘é‡è¿›åº¦: {min(i + embedding_batch_size, len(new_docs_for_embedding))}/{len(new_docs_for_embedding)}")

            db_batch_size = 4000
            total_batches = (len(new_ids) + db_batch_size - 1) // db_batch_size
            print(f"\nâ• æ­£åœ¨å°† {len(new_ids)} ä¸ªæ–‡æ¡£å—åˆ† {total_batches} æ‰¹æ·»åŠ åˆ°æ•°æ®åº“...")

            for i in range(0, len(new_ids), db_batch_size):
                batch_ids = new_ids[i:i + db_batch_size]
                batch_documents = new_docs_content[i:i + db_batch_size]
                batch_embeddings = embeddings[i:i + db_batch_size]
                batch_metadatas = new_metadatas[i:i + db_batch_size]

                self.collection.add(
                    ids=batch_ids,
                    documents=batch_documents,
                    embeddings=batch_embeddings,
                    metadatas=batch_metadatas
                )
                print(f"   æ‰¹æ¬¡ {i // db_batch_size + 1}/{total_batches} æ·»åŠ æˆåŠŸ")

            print(f"âœ… æˆåŠŸæ·»åŠ  {len(new_docs_content)} ä¸ªæ–‡æ¡£å—")

        # ========== æœ€ç»ˆç»Ÿè®¡ ==========
        print(f"\nâœ… å¢é‡æ›´æ–°å®Œæˆ!")
        print(f"ğŸ“Š æ•°æ®åº“å½“å‰å…±æœ‰ {self.collection.count()} ä¸ªæ–‡æ¡£å—")

    def search(self, query: str, n_results: Optional[int] = None) -> dict:
        """æ··åˆæ£€ç´¢"""
        query_embedding = self._get_embedding(query)
        n_results = n_results or self.config.max_results
        search_n = min(n_results * 5, self.collection.count())

        semantic_results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=search_n,
            include=["metadatas", "documents", "distances"]
        )

        if not semantic_results['documents'][0]:
            return semantic_results

        if self.config.use_hybrid_search:
            scored_results = []
            for doc_id, doc, meta, dist in zip(
                    semantic_results['ids'][0],
                    semantic_results['documents'][0],
                    semantic_results['metadatas'][0],
                    semantic_results['distances'][0]
            ):
                semantic_score = 1 - dist
                keyword_score = self._keyword_match_score(query, doc, meta.get('keywords', ''))
                final_score = semantic_score * (
                        1 - self.config.keyword_boost) + keyword_score * self.config.keyword_boost
                scored_results.append({
                    'id': doc_id,
                    'doc': doc,
                    'meta': meta,
                    'dist': dist,
                    'score': final_score
                })

            scored_results.sort(key=lambda x: x['score'], reverse=True)
            filtered = [r for r in scored_results if (1 - r['dist']) >= self.config.similarity_threshold]
            top_results = filtered[:n_results]

            return {
                'ids': [[r['id'] for r in top_results]],
                'documents': [[r['doc'] for r in top_results]],
                'metadatas': [[r['meta'] for r in top_results]],
                'distances': [[r['dist'] for r in top_results]],
            }

        indices = [i for i, d in enumerate(semantic_results['distances'][0]) if
                   (1 - d) >= self.config.similarity_threshold]
        top_indices = indices[:n_results]
        return {
            'ids': [[semantic_results['ids'][0][i] for i in top_indices]],
            'documents': [[semantic_results['documents'][0][i] for i in top_indices]],
            'metadatas': [[semantic_results['metadatas'][0][i] for i in top_indices]],
            'distances': [[semantic_results['distances'][0][i] for i in top_indices]],
        }

    def expand_context_with_window(self, search_results: dict) -> List[Dict]:
        """æ‰©å±•æ£€ç´¢ç»“æœçš„ä¸Šä¸‹æ–‡çª—å£"""
        if self.config.context_window_size <= 1 or not search_results['ids'][0]:
            return [{"doc": doc, "meta": meta, "is_hit": True} for doc, meta in
                    zip(search_results['documents'][0], search_results['metadatas'][0])]

        print("ğŸ”„ æ­£åœ¨æ‰©å±•ä¸Šä¸‹æ–‡çª—å£...")
        final_docs = {
            r_id: {"doc": doc, "meta": meta, "is_hit": True}
            for r_id, doc, meta in zip(
                search_results['ids'][0],
                search_results['documents'][0],
                search_results['metadatas'][0]
            )
        }

        ids_to_fetch = set()
        window_radius = self.config.context_window_size // 2

        for r_id in search_results['ids'][0]:
            parts = r_id.rsplit('_chunk_', 1)
            if len(parts) == 2 and parts[1].isdigit():
                base_name, index = parts[0], int(parts[1])
                for i in range(1, window_radius + 1):
                    prev_id = f"{base_name}_chunk_{index - i}"
                    next_id = f"{base_name}_chunk_{index + i}"
                    if prev_id not in final_docs:
                        ids_to_fetch.add(prev_id)
                    if next_id not in final_docs:
                        ids_to_fetch.add(next_id)

        if ids_to_fetch:
            ids_list = list(ids_to_fetch)
            get_batch_size = 500
            for i in range(0, len(ids_list), get_batch_size):
                batch_ids = ids_list[i:i + get_batch_size]
                context_docs = self.collection.get(ids=batch_ids, include=["metadatas", "documents"])
                for c_id, doc, meta in zip(context_docs['ids'], context_docs['documents'], context_docs['metadatas']):
                    if c_id not in final_docs:
                        final_docs[c_id] = {"doc": doc, "meta": meta, "is_hit": False}

        sorted_ids = sorted(
            final_docs.keys(),
            key=lambda x: (final_docs[x]['meta']['source'], int(x.rsplit('_', 1)[1]))
        )
        return [final_docs[id] for id in sorted_ids]

    def get_stats(self) -> dict:
        """è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        count = self.collection.count()
        results = self.collection.get()
        docs_by_source = defaultdict(list)

        for metadata in results['metadatas']:
            source = metadata.get('source', 'unknown')
            docs_by_source[source].append(metadata)

        doc_list = []
        for source, metadatas in sorted(docs_by_source.items()):
            doc_list.append({
                'source': source,
                'chunk_count': len(metadatas),
            })

        return {
            'total_chunks': count,
            'total_documents': len(doc_list),
            'documents': doc_list
        }