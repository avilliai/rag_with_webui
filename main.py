"""
æ”¹è¿›çš„ Gemini RAG ç³»ç»Ÿ - æ··åˆæ£€ç´¢ç­–ç•¥
æ ¸å¿ƒæ”¹è¿›: å…³é”®è¯åŒ¹é… + è¯­ä¹‰æ£€ç´¢ åŒé‡ä¿éšœ
ä¸“é—¨ä¼˜åŒ–äººåã€ä¸“æœ‰åè¯çš„æ£€ç´¢å‡†ç¡®åº¦
"""
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import Optional, Dict
import asyncio
import json
import uuid
from datetime import datetime

from flask import Flask, request, jsonify
from flask_cors import CORS
import hashlib
import os
from pathlib import Path
from typing import List, Optional, Tuple, Dict, Set
from sentence_transformers import SentenceTransformer
import chromadb
import google.generativeai as genai
import re
from collections import defaultdict

API_KEY = 'AIzaSyCNwmo17IETTpEAhCp9mvrtaovXteITZDM'

genai.configure(api_key=API_KEY)
os.environ["http_proxy"] = "http://127.0.0.1:10809"
os.environ["https_proxy"] = "http://127.0.0.1:10809"


class RAGConfig:
    """RAG ç³»ç»Ÿé…ç½®"""
    def __init__(
        self,
        max_results: int = 8,
        similarity_threshold: Optional[float] = None,
        chunk_size: int = 1000,  # å¢å¤§ä»¥ä¿ç•™æ›´å¤šä¸Šä¸‹æ–‡
        min_chunk_size: int = 200,
        chunk_overlap: int = 250,  # å¢åŠ é‡å 
        use_hybrid_search: bool = True,  # å¯ç”¨æ··åˆæ£€ç´¢
        keyword_boost: float = 0.3  # å…³é”®è¯åŒ¹é…çš„æƒé‡æå‡
    ):
        self.max_results = max_results
        self.similarity_threshold = similarity_threshold
        self.chunk_size = chunk_size
        self.min_chunk_size = min_chunk_size
        self.chunk_overlap = chunk_overlap
        self.use_hybrid_search = use_hybrid_search
        self.keyword_boost = keyword_boost


class GeminiRAG:
    def __init__(
        self,
        collection_name: str = "documents",
        persist_directory: str = "./chroma_db",
        config: Optional[RAGConfig] = None
    ):
        """åˆå§‹åŒ– RAG ç³»ç»Ÿ"""
        self.persist_directory = persist_directory
        self.config = config or RAGConfig()

        print(f"ğŸ“¦ åˆå§‹åŒ– ChromaDB (å­˜å‚¨è·¯å¾„: {persist_directory})...")
        self.client = chromadb.PersistentClient(path=persist_directory)

        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            metadata={"hnsw:space": "cosine"}
        )

        existing_count = self.collection.count()
        if existing_count > 0:
            print(f"âœ… å‘ç°å·²å­˜åœ¨çš„æ•°æ®åº“,åŒ…å« {existing_count} ä¸ªæ–‡æ¡£å—")

        print("ğŸ“¦ åŠ è½½åµŒå…¥æ¨¡å‹...")
        self.embed_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        print("âœ… åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆ")

        self.chat_model = genai.GenerativeModel('gemini-2.5-flash')

        print(f"\nâš™ï¸  RAG é…ç½®:")
        print(f"   â€¢ æ£€ç´¢ç­–ç•¥: {'æ··åˆæ£€ç´¢ (å…³é”®è¯+è¯­ä¹‰)' if self.config.use_hybrid_search else 'çº¯è¯­ä¹‰æ£€ç´¢'}")
        print(f"   â€¢ æœ€å¤§æ£€ç´¢æ•°: {self.config.max_results}")
        print(f"   â€¢ ç›¸ä¼¼åº¦é˜ˆå€¼: {self.config.similarity_threshold or 'è‡ªåŠ¨'}")
        print(f"   â€¢ åˆ†å—å¤§å°: {self.config.chunk_size} å­—ç¬¦")
        print(f"   â€¢ é‡å å¤§å°: {self.config.chunk_overlap} å­—ç¬¦")
        if self.config.use_hybrid_search:
            print(f"   â€¢ å…³é”®è¯æƒé‡: {self.config.keyword_boost}")

    def _get_embedding(self, text: str) -> List[float]:
        """ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡"""
        embedding = self.embed_model.encode(text, convert_to_tensor=False)
        return embedding.tolist()

    def _get_file_hash(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶çš„ MD5 å“ˆå¸Œå€¼"""
        with open(file_path, 'rb') as f:
            return hashlib.md5(f.read()).hexdigest()

    def _read_markdown_file(self, file_path: Path) -> str:
        """è¯»å– Markdown æ–‡ä»¶å†…å®¹"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            print(f"âš ï¸  è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return None

    def _extract_keywords(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯"""
        keywords = []

        # æå– #æ ‡ç­¾
        hashtags = re.findall(r'#([^\s#]+)', text)
        keywords.extend(hashtags)

        # æå–åŠ ç²—çš„æ–‡æœ¬
        bold_text = re.findall(r'\*\*([^*]+)\*\*', text)
        keywords.extend([t.strip() for t in bold_text if 3 < len(t.strip()) < 30])

        # æå–æ ‡é¢˜ä¸­çš„å…³é”®è¯
        headers = re.findall(r'^#{1,6}\s+(.+)$', text, re.MULTILINE)
        keywords.extend([h.strip() for h in headers if len(h.strip()) < 50])

        # æå–äººåæ¨¡å¼ï¼ˆä¸­æ–‡å§“åï¼Œé€šå¸¸2-4ä¸ªå­—ï¼‰
        # åŒ¹é…å¸¸è§çš„å­¦è€…åå­—æ¨¡å¼
        names = re.findall(r'([Â·\u4e00-\u9fa5]{2,6}(?:çš„|ã€|ä¸|å’Œ|åŠ)?)', text)
        potential_names = [n.strip('çš„ã€ä¸å’ŒåŠ') for n in names
                          if 2 <= len(n.strip('çš„ã€ä¸å’ŒåŠ')) <= 6]
        keywords.extend(potential_names)

        return list(set(keywords))

    def _normalize_text(self, text: str) -> str:
        """æ–‡æœ¬å½’ä¸€åŒ–ï¼šå»é™¤å¤šä½™ç©ºæ ¼ã€ç»Ÿä¸€æ ‡ç‚¹ç­‰"""
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text)
        # ç»Ÿä¸€å¼•å·
        text = text.replace('"', '"').replace('"', '"')
        text = text.replace(''', "'").replace(''', "'")
        return text.strip()

    def _extract_query_keywords(self, query: str) -> List[str]:
        """ä»æŸ¥è¯¢ä¸­æå–å…³é”®è¯"""
        keywords = []

        # æå–å¯èƒ½çš„äººåï¼ˆ2-6ä¸ªæ±‰å­—ï¼‰
        names = re.findall(r'[\u4e00-\u9fa5Â·]{2,6}', query)
        keywords.extend(names)

        # æå–å¸¸è§çš„æŸ¥è¯¢å…³é”®è¯
        key_phrases = ['æ”¿æ²»æ€æƒ³', 'ç†è®º', 'è§‚ç‚¹', 'å­¦è¯´', 'ä¸»å¼ ', 'æ‰¹åˆ¤', 'è¯„ä»·']
        for phrase in key_phrases:
            if phrase in query:
                keywords.append(phrase)

        return keywords

    def _keyword_match_score(self, query: str, doc_text: str, doc_keywords: str) -> float:
        """è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°"""
        query_lower = query.lower()
        doc_text_lower = doc_text.lower()
        doc_keywords_lower = doc_keywords.lower() if doc_keywords else ""

        score = 0.0
        query_keywords = self._extract_query_keywords(query)

        for keyword in query_keywords:
            keyword_lower = keyword.lower()
            # ç²¾ç¡®åŒ¹é…å…³é”®è¯
            if keyword_lower in doc_keywords_lower:
                score += 0.5  # å…³é”®è¯å­—æ®µåŒ¹é…
            if keyword_lower in doc_text_lower:
                # è®¡ç®—å‡ºç°æ¬¡æ•°ï¼ˆä¸Šé™ä¸º5æ¬¡ï¼‰
                count = min(doc_text_lower.count(keyword_lower), 5)
                score += count * 0.1  # æ–‡æœ¬ä¸­åŒ¹é…

        return min(score, 1.0)  # é™åˆ¶åœ¨0-1ä¹‹é—´

    def _parse_document_structure(self, text: str) -> List[Dict]:
        """è§£ææ–‡æ¡£ç»“æ„"""
        lines = text.split('\n')
        sections = []
        current_section = {
            'content': [],
            'headers': [],
            'line_start': 0
        }
        header_stack = []

        for i, line in enumerate(lines):
            line_stripped = line.strip()

            # æ£€æµ‹æ ‡é¢˜
            header_match = re.match(r'^(#{1,6})\s+(.+)$', line_stripped)
            if header_match:
                # ä¿å­˜å½“å‰section
                if current_section['content']:
                    current_section['content'] = '\n'.join(current_section['content'])
                    current_section['line_end'] = i
                    sections.append(current_section.copy())

                # æ›´æ–°æ ‡é¢˜æ ˆ
                level = len(header_match.group(1))
                title = header_match.group(2).strip()

                while header_stack and header_stack[-1]['level'] >= level:
                    header_stack.pop()
                header_stack.append({'level': level, 'title': title})

                # å¼€å§‹æ–°section
                current_section = {
                    'content': [line],
                    'headers': [h['title'] for h in header_stack],
                    'header_level': level,
                    'line_start': i
                }
            elif line_stripped:
                current_section['content'].append(line)
            elif len(current_section['content']) > 5:  # ç©ºè¡Œä¸”æœ‰è¶³å¤Ÿå†…å®¹
                # ä¿å­˜section
                current_section['content'] = '\n'.join(current_section['content'])
                current_section['line_end'] = i
                current_section['headers'] = [h['title'] for h in header_stack]
                sections.append(current_section.copy())

                # å¼€å§‹æ–°section
                current_section = {
                    'content': [],
                    'headers': [h['title'] for h in header_stack],
                    'line_start': i + 1
                }

        # ä¿å­˜æœ€åä¸€ä¸ªsection
        if current_section['content']:
            if isinstance(current_section['content'], list):
                current_section['content'] = '\n'.join(current_section['content'])
            current_section['line_end'] = len(lines)
            current_section['headers'] = [h['title'] for h in header_stack]
            sections.append(current_section)

        return sections

    def _smart_chunk_document(self, text: str, file_name: str) -> List[tuple]:
        """æ™ºèƒ½åˆ†å—æ–‡æ¡£"""
        sections = self._parse_document_structure(text)
        chunks = []
        chunk_idx = 0

        i = 0
        while i < len(sections):
            section = sections[i]

            # æ„å»ºæ ‡é¢˜ä¸Šä¸‹æ–‡
            headers_text = ' > '.join(section.get('headers', []))
            header_prefix = f"# {headers_text}\n\n" if headers_text else ""

            # æ”¶é›†å†…å®¹
            chunk_content = []
            chunk_sections = []
            current_size = len(header_prefix)

            j = i
            while j < len(sections):
                candidate = sections[j]
                content = candidate['content']
                content_size = len(content)

                # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°çš„ä¸»è¦ä¸»é¢˜ï¼ˆ## æˆ– # çº§åˆ«ï¼‰
                if (j > i and
                    candidate.get('header_level') and
                    candidate['header_level'] <= 2):
                    break

                # æ£€æŸ¥å¤§å°é™åˆ¶
                if current_size + content_size > self.config.chunk_size and chunk_content:
                    break

                chunk_content.append(content)
                chunk_sections.append(j)
                current_size += content_size + 2
                j += 1

            # å¦‚æœæ²¡æœ‰æ”¶é›†åˆ°å†…å®¹ï¼ˆå•ä¸ªsectionè¿‡å¤§ï¼‰
            if not chunk_content and i < len(sections):
                content = sections[i]['content']
                # åˆ†å‰²é•¿å†…å®¹
                parts = self._split_long_text(content, self.config.chunk_size - len(header_prefix))
                for part_idx, part in enumerate(parts):
                    full_text = header_prefix + part
                    keywords = self._extract_keywords(full_text)

                    chunk_meta = {
                        'section_path': headers_text,
                        'keywords': ', '.join(keywords),
                        'section_indices': str(i),
                        'is_split': True,
                        'part': f"{part_idx + 1}/{len(parts)}"
                    }

                    chunks.append((full_text, f"{file_name}_chunk_{chunk_idx}",
                                 len(full_text), chunk_meta))
                    chunk_idx += 1
                i += 1
                continue

            # æ„å»ºå®Œæ•´chunk
            full_content = '\n\n'.join(chunk_content)
            full_text = header_prefix + full_content
            keywords = self._extract_keywords(full_text)

            chunk_meta = {
                'section_path': headers_text,
                'keywords': ', '.join(keywords),
                'section_indices': f"{min(chunk_sections)}-{max(chunk_sections)}" if len(chunk_sections) > 1 else str(chunk_sections[0]),
                'section_count': len(chunk_sections),
                'is_split': False
            }

            chunks.append((full_text, f"{file_name}_chunk_{chunk_idx}",
                         len(full_text), chunk_meta))
            chunk_idx += 1

            # å†³å®šé‡å ç­–ç•¥
            if len(chunk_sections) > 2:
                # ä»å€’æ•°ç¬¬äºŒä¸ªsectionå¼€å§‹é‡å 
                i = chunk_sections[-2]
            else:
                i = j

        return chunks

    def _split_long_text(self, text: str, max_size: int) -> List[str]:
        """åˆ†å‰²è¶…é•¿æ–‡æœ¬"""
        if len(text) <= max_size:
            return [text]

        parts = []
        # æŒ‰å¥å­åˆ†å‰²
        sentences = re.split(r'([ã€‚ï¼ï¼Ÿ\n]+)', text)

        current = ""
        for i in range(0, len(sentences), 2):
            sentence = sentences[i]
            delimiter = sentences[i + 1] if i + 1 < len(sentences) else ""

            if len(current) + len(sentence) + len(delimiter) > max_size and current:
                parts.append(current)
                # ä¿ç•™ä¸€äº›é‡å 
                overlap = current[-self.config.chunk_overlap:] if len(current) > self.config.chunk_overlap else current
                current = overlap + sentence + delimiter
            else:
                current += sentence + delimiter

        if current:
            parts.append(current)

        return parts if parts else [text[:max_size]]

    def load_documents_from_folder(self, folder_path: str = "./docs", force_reload: bool = False):
        """ä»æ–‡ä»¶å¤¹é€’å½’åŠ è½½æ‰€æœ‰ Markdown æ–‡æ¡£"""
        docs_path = Path(folder_path)

        if not docs_path.exists():
            print(f"âŒ æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
            return

        md_files = list(docs_path.rglob("*.md"))

        if not md_files:
            print(f"âš ï¸  åœ¨ {folder_path} ä¸­æ²¡æœ‰æ‰¾åˆ° Markdown æ–‡ä»¶")
            return

        print(f"\nğŸ“ æ‰¾åˆ° {len(md_files)} ä¸ª Markdown æ–‡ä»¶")

        existing_docs = self.collection.get()
        existing_ids = set(existing_docs['ids']) if existing_docs['ids'] else set()

        existing_hashes = {}
        if existing_docs['metadatas']:
            for metadata in existing_docs['metadatas']:
                source = metadata.get('source', '')
                file_hash = metadata.get('file_hash', '')
                if source and file_hash and source not in existing_hashes:
                    existing_hashes[source] = file_hash

        print(f"ğŸ“‹ æ•°æ®åº“ä¸­å·²æœ‰ {len(existing_hashes)} ä¸ªä¸åŒçš„æ–‡æ¡£æ–‡ä»¶")

        new_docs = []
        new_ids = []
        new_metadatas = []
        updated_count = 0
        skipped_count = 0
        new_count = 0

        for md_file in md_files:
            relative_path = md_file.relative_to(docs_path)
            source_path = str(relative_path).replace('\\', '/')
            current_hash = self._get_file_hash(str(md_file))

            if source_path in existing_hashes:
                if not force_reload and existing_hashes[source_path] == current_hash:
                    skipped_count += 1
                    continue
                else:
                    print(f"ğŸ”„ æ£€æµ‹åˆ°æ–‡ä»¶å˜æ›´: {source_path}")
                    updated_count += 1
                    ids_to_delete = [eid for eid in existing_ids
                                    if eid.startswith(f"{source_path}_chunk_")]
                    if ids_to_delete:
                        self.collection.delete(ids=ids_to_delete)
            else:
                new_count += 1

            content = self._read_markdown_file(md_file)

            if content:
                chunks = self._smart_chunk_document(content, source_path)

                # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼ˆå‰3ä¸ªæ–‡ä»¶ï¼‰
                if len(new_docs) < 50:
                    print(f"ğŸ“„ {source_path}: {len(content)} å­—ç¬¦ â†’ {len(chunks)} å—")

                for chunk_text, chunk_id, chunk_size, chunk_meta in chunks:
                    new_docs.append(chunk_text)
                    new_ids.append(chunk_id)

                    metadata = {
                        "source": source_path,
                        "file_name": md_file.name,
                        "file_hash": current_hash,
                        "file_size": md_file.stat().st_size,
                        "chunk_id": chunk_id,
                        "chunk_size": chunk_size,
                        **chunk_meta
                    }
                    new_metadatas.append(metadata)

        if new_docs:
            print(f"\nğŸ’¾ æ­£åœ¨æ·»åŠ  {len(new_docs)} ä¸ªæ–‡æ¡£å—...")
            print(f"   ğŸ“ æ–°å¢: {new_count} ä¸ªæ–‡ä»¶")
            if updated_count > 0:
                print(f"   ğŸ”„ æ›´æ–°: {updated_count} ä¸ªæ–‡ä»¶")

            print("ğŸ”„ ç”ŸæˆåµŒå…¥å‘é‡ï¼ˆå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...")
            # æ‰¹é‡ç”ŸæˆåµŒå…¥ä»¥æé«˜æ•ˆç‡
            batch_size = 100
            embeddings = []
            for i in range(0, len(new_docs), batch_size):
                batch = new_docs[i:i+batch_size]
                batch_embeddings = [self._get_embedding(doc) for doc in batch]
                embeddings.extend(batch_embeddings)
                if len(new_docs) > batch_size:
                    print(f"   è¿›åº¦: {min(i+batch_size, len(new_docs))}/{len(new_docs)}")

            self.collection.add(
                documents=new_docs,
                embeddings=embeddings,
                ids=new_ids,
                metadatas=new_metadatas
            )

            print(f"âœ… æˆåŠŸæ·»åŠ  {len(new_docs)} ä¸ªæ–‡æ¡£å—")
        else:
            print(f"\nâœ… æ‰€æœ‰æ–‡æ¡£å·²æ˜¯æœ€æ–°")

        if skipped_count > 0:
            print(f"â­ï¸  è·³è¿‡ {skipped_count} ä¸ªæœªä¿®æ”¹çš„æ–‡æ¡£")

        print(f"\nğŸ“Š æ•°æ®åº“å…±æœ‰ {self.collection.count()} ä¸ªæ–‡æ¡£å—")

    def search(self, query: str, n_results: Optional[int] = None,
               similarity_threshold: Optional[float] = None) -> dict:
        """æ··åˆæ£€ç´¢ï¼šå…³é”®è¯åŒ¹é… + è¯­ä¹‰ç›¸ä¼¼åº¦"""
        n_results = n_results or self.config.max_results
        similarity_threshold = similarity_threshold if similarity_threshold is not None else self.config.similarity_threshold

        # è¯­ä¹‰æ£€ç´¢
        query_embedding = self._get_embedding(query)
        search_n = min(n_results * 5, self.collection.count())  # æ£€ç´¢æ›´å¤šå€™é€‰

        semantic_results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=search_n
        )

        if not semantic_results or not semantic_results.get('documents') or not semantic_results['documents'][0]:
            return {'ids': [[]], 'documents': [[]], 'metadatas': [[]], 'distances': [[]]}

        # å¦‚æœå¯ç”¨æ··åˆæ£€ç´¢ï¼Œé‡æ–°æ’åº
        if self.config.use_hybrid_search:
            # è®¡ç®—æ··åˆåˆ†æ•°
            scored_results = []
            for i, (doc_id, doc, metadata, distance) in enumerate(zip(
                semantic_results['ids'][0],
                semantic_results['documents'][0],
                semantic_results['metadatas'][0],
                semantic_results['distances'][0]
            )):
                # è¯­ä¹‰åˆ†æ•°ï¼ˆdistanceè¶Šå°è¶Šå¥½ï¼Œè½¬æ¢ä¸ºç›¸ä¼¼åº¦ï¼‰
                semantic_score = 1 - distance

                # å…³é”®è¯åŒ¹é…åˆ†æ•°
                doc_keywords = metadata.get('keywords', '')
                keyword_score = self._keyword_match_score(query, doc, doc_keywords)

                # æ··åˆåˆ†æ•°
                final_score = semantic_score * (1 - self.config.keyword_boost) + \
                             keyword_score * self.config.keyword_boost

                scored_results.append({
                    'id': doc_id,
                    'document': doc,
                    'metadata': metadata,
                    'distance': distance,
                    'semantic_score': semantic_score,
                    'keyword_score': keyword_score,
                    'final_score': final_score
                })

            # æŒ‰æœ€ç»ˆåˆ†æ•°æ’åº
            scored_results.sort(key=lambda x: x['final_score'], reverse=True)

            # åº”ç”¨é˜ˆå€¼è¿‡æ»¤ï¼ˆå¦‚æœè®¾ç½®äº†ï¼‰
            if similarity_threshold is not None:
                # å°†é˜ˆå€¼åº”ç”¨äºsemantic distance
                scored_results = [r for r in scored_results if r['distance'] <= similarity_threshold]

            # é™åˆ¶ç»“æœæ•°é‡
            scored_results = scored_results[:n_results]

            # æ„å»ºè¿”å›æ ¼å¼
            return {
                'ids': [[r['id'] for r in scored_results]],
                'documents': [[r['document'] for r in scored_results]],
                'metadatas': [[r['metadata'] for r in scored_results]],
                'distances': [[r['distance'] for r in scored_results]],
                'scores': [[r['final_score'] for r in scored_results]],  # é¢å¤–è¿”å›æ··åˆåˆ†æ•°
                'keyword_scores': [[r['keyword_score'] for r in scored_results]]
            }

        # éæ··åˆæ£€ç´¢ï¼Œç›´æ¥è¿”å›è¯­ä¹‰ç»“æœ
        if similarity_threshold is not None:
            filtered_indices = [i for i, dist in enumerate(semantic_results['distances'][0])
                              if dist <= similarity_threshold]
            filtered_indices = filtered_indices[:n_results]

            return {
                'ids': [[semantic_results['ids'][0][i] for i in filtered_indices]],
                'documents': [[semantic_results['documents'][0][i] for i in filtered_indices]],
                'metadatas': [[semantic_results['metadatas'][0][i] for i in filtered_indices]],
                'distances': [[semantic_results['distances'][0][i] for i in filtered_indices]]
            }

        # é™åˆ¶æ•°é‡
        if len(semantic_results['ids'][0]) > n_results:
            return {
                'ids': [semantic_results['ids'][0][:n_results]],
                'documents': [semantic_results['documents'][0][:n_results]],
                'metadatas': [semantic_results['metadatas'][0][:n_results]],
                'distances': [semantic_results['distances'][0][:n_results]]
            }

        return semantic_results

    def generate_answer_stream(self, query: str, chat_history: list = None,
                               n_results: Optional[int] = None,
                               similarity_threshold: Optional[float] = None):
        """RAG æµå¼ç”Ÿæˆç­”æ¡ˆï¼ˆæ”¯æŒå¤šè½®å¯¹è¯ï¼‰"""
        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        search_results = self.search(query, n_results, similarity_threshold)

        has_results = (
                search_results and
                search_results.get('documents') and
                len(search_results['documents']) > 0 and
                len(search_results['documents'][0]) > 0
        )

        if not has_results:
            yield json.dumps({
                'type': 'error',
                'content': 'æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£,æ— æ³•å›ç­”é—®é¢˜ã€‚'
            }, ensure_ascii=False) + '\n'
            return

        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        sources_info = []

        for i, (doc, metadata, distance) in enumerate(zip(
                search_results['documents'][0],
                search_results['metadatas'][0],
                search_results['distances'][0]
        )):
            source = metadata.get('source', f'æ–‡æ¡£{i + 1}')
            section_path = metadata.get('section_path', '')
            keywords = metadata.get('keywords', '')
            semantic_score = 1 - distance
            keyword_score = search_results.get('keyword_scores', [[]])[0][
                i] if 'keyword_scores' in search_results else 0

            source_info = f"[æ¥æº: {source}"
            if section_path:
                source_info += f" | ç« èŠ‚: {section_path}"
            if keywords:
                kw_list = keywords.split(', ')[:5]
                source_info += f" | å…³é”®è¯: {', '.join(kw_list)}"
            source_info += f" | è¯­ä¹‰: {semantic_score:.2%}"
            if keyword_score > 0:
                source_info += f" | å…³é”®è¯: {keyword_score:.2%}"
            source_info += "]"

            context_parts.append(f"{source_info}\n{doc}")

            sources_info.append({
                'source': source,
                'section_path': section_path,
                'keywords': keywords.split(', ')[:5] if keywords else [],
                'semantic_similarity': round(semantic_score * 100, 2),
                'keyword_score': round(keyword_score * 100, 2)
            })

        context = "\n\n---\n\n".join(context_parts)

        # å…ˆå‘é€æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¿¡æ¯
        yield json.dumps({
            'type': 'sources',
            'content': sources_info,
            'count': len(sources_info)
        }, ensure_ascii=False) + '\n'

        # æ„å»ºåŒ…å«å†å²çš„ prompt
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ”¿æ²»å­¦çŸ¥è¯†è§£ç­”æ¨¡å‹ï¼Œä½ å¿…é¡»åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ã€‚
    ç»™å‡ºç³»ç»Ÿã€å­¦æœ¯åŒ–çš„è§£ç­”ã€‚ä½ ä¸è¢«å…è®¸é—æ¼ä»»ä½•æ–‡æ¡£ä¸­çš„ä¿¡æ¯ã€‚
    å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯,è¯·è¯´æ˜æ— æ³•å›ç­”ã€‚"""

        # åˆ›å»ºæˆ–ä½¿ç”¨ç°æœ‰çš„ chat ä¼šè¯
        if chat_history:
            # ä½¿ç”¨ genai çš„æ–°æ¥å£åˆ›å»º chat
            chat = self.chat_model.start_chat(history=chat_history)
        else:
            chat = self.chat_model.start_chat()

        # æ„å»ºç”¨æˆ·æ¶ˆæ¯
        user_message = f"""æ£€ç´¢åˆ°çš„æ–‡æ¡£:
    {context}

    é—®é¢˜: {query}

    è¯·æä¾›è¯¦ç»†ä¸”å‡†ç¡®çš„ç­”æ¡ˆ:"""

        # æµå¼ç”Ÿæˆ
        try:
            response = chat.send_message(user_message, stream=True)

            for chunk in response:
                if chunk.text:
                    yield json.dumps({
                        'type': 'content',
                        'content': chunk.text
                    }, ensure_ascii=False) + '\n'

            # å‘é€å®Œæˆä¿¡å·
            yield json.dumps({
                'type': 'done',
                'content': ''
            }, ensure_ascii=False) + '\n'

        except Exception as e:
            yield json.dumps({
                'type': 'error',
                'content': f'ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {str(e)}'
            }, ensure_ascii=False) + '\n'
    def generate_answer(self, query: str, n_results: Optional[int] = None,
                       similarity_threshold: Optional[float] = None) -> str:
        """RAG: æ£€ç´¢ç›¸å…³æ–‡æ¡£å¹¶ç”Ÿæˆç­”æ¡ˆ"""
        search_results = self.search(query, n_results, similarity_threshold)

        has_results = (
            search_results and
            search_results.get('documents') and
            len(search_results['documents']) > 0 and
            len(search_results['documents'][0]) > 0
        )

        if not has_results:
            return "âŒ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£,æ— æ³•å›ç­”é—®é¢˜ã€‚"

        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        for i, (doc, metadata, distance) in enumerate(zip(
                search_results['documents'][0],
                search_results['metadatas'][0],
                search_results['distances'][0]
        )):
            source = metadata.get('source', f'æ–‡æ¡£{i + 1}')
            section_path = metadata.get('section_path', '')
            keywords = metadata.get('keywords', '')
            semantic_score = 1 - distance

            # å¦‚æœæœ‰æ··åˆåˆ†æ•°ï¼Œä¹Ÿæ˜¾ç¤º
            keyword_score = search_results.get('keyword_scores', [[]])[0][i] if 'keyword_scores' in search_results else 0

            source_info = f"[æ¥æº: {source}"
            if section_path:
                source_info += f" | ç« èŠ‚: {section_path}"
            if keywords:
                kw_list = keywords.split(', ')[:5]
                source_info += f" | å…³é”®è¯: {', '.join(kw_list)}"
            source_info += f" | è¯­ä¹‰: {semantic_score:.2%}"
            if keyword_score > 0:
                source_info += f" | å…³é”®è¯: {keyword_score:.2%}"
            source_info += "]"

            context_parts.append(f"{source_info}\n{doc}")

        context = "\n\n---\n\n".join(context_parts)

        print(f"\nğŸ” æ£€ç´¢åˆ° {len(context_parts)} ä¸ªç›¸å…³æ–‡æ¡£å—")

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ”¿æ²»å­¦çŸ¥è¯†è§£ç­”æ¨¡å‹ï¼Œä½ å¿…é¡»åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ã€‚ç»™å‡ºç³»ç»Ÿã€å­¦æœ¯åŒ–çš„è§£ç­”ã€‚ä½ ä¸è¢«å…è®¸é—æ¼ä»»ä½•æ–‡æ¡£ä¸­çš„ä¿¡æ¯ã€‚å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯,è¯·è¯´æ˜æ— æ³•å›ç­”ã€‚

æ£€ç´¢åˆ°çš„æ–‡æ¡£:
{context}

é—®é¢˜: {query}

è¯·æä¾›è¯¦ç»†ä¸”å‡†ç¡®çš„ç­”æ¡ˆ:"""

        try:
            response = self.chat_model.generate_content(prompt)
            return response.text
        except Exception as e:
            return f"ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {e}"

    def get_collection_info(self):
        """è·å–é›†åˆä¿¡æ¯"""
        count = self.collection.count()
        return f"é›†åˆä¸­å…±æœ‰ {count} ä¸ªæ–‡æ¡£å—"

    def list_documents(self, show_sample: bool = False):
        """åˆ—å‡ºæ‰€æœ‰æ–‡æ¡£"""
        results = self.collection.get()
        print(f"\nğŸ“š æ•°æ®åº“ä¸­çš„æ–‡æ¡£åˆ—è¡¨:")
        print("=" * 80)

        docs_by_source = defaultdict(list)
        for doc_id, metadata in zip(results['ids'], results['metadatas']):
            source = metadata.get('source', doc_id)
            docs_by_source[source].append(metadata)

        for i, (source, metadatas) in enumerate(sorted(docs_by_source.items()), 1):
            file_size = metadatas[0].get('file_size', 0)
            chunk_count = len(metadatas)
            total_chunk_size = sum(m.get('chunk_size', 0) for m in metadatas)
            avg_chunk_size = total_chunk_size // chunk_count if chunk_count > 0 else 0

            print(f"\n{i}. {source}")
            print(f"   å¤§å°: {file_size:,} bytes â†’ {chunk_count} å— (å¹³å‡ {avg_chunk_size} å­—ç¬¦/å—)")

            if show_sample and i <= 3:
                # æ˜¾ç¤ºå‰3ä¸ªå—çš„å…³é”®è¯
                for j, meta in enumerate(metadatas[:3]):
                    keywords = meta.get('keywords', '')
                    if keywords:
                        kw_list = keywords.split(', ')[:8]
                        print(f"   å—{j}: {', '.join(kw_list)}")

        print("=" * 80)


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ åˆå§‹åŒ–æ··åˆæ£€ç´¢ RAG ç³»ç»Ÿ...")

    config = RAGConfig(
        max_results=10,
        similarity_threshold=0.70,  # å¯é€‚å½“è°ƒæ•´
        chunk_size=1000,
        chunk_overlap=250,
        use_hybrid_search=True,
        keyword_boost=0.3  # 30%æƒé‡ç»™å…³é”®è¯åŒ¹é…
    )
    rag = GeminiRAG(
        collection_name="md_documents",
        persist_directory="./chroma_db",
        config=config
    )

    print("\nğŸ“– ä» docs æ–‡ä»¶å¤¹åŠ è½½æ–‡æ¡£...")
    rag.load_documents_from_folder(
        folder_path="./docs",
        force_reload=False,
    )

    rag.list_documents()
    print(f"\nğŸ“Š {rag.get_collection_info()}")

    print("\n" + "=" * 70)
    print("ğŸ¤– å¼€å§‹æµ‹è¯•æŸ¥è¯¢...")
    print("=" * 70)

    queries = [
        "å¥¥å…‹è‚–ç‰¹çš„æ”¿æ²»æ€æƒ³",
        "å¦‚ä½•è¯„ä»·ç½—å°”æ–¯",
        "æ ¼æ—çš„æ”¿æ²»æ€æƒ³",
        "è¯ºé½å…‹çš„ç†è®ºä»‹ç»",
    ]

    for query in queries:
        print(f"\nâ“ é—®é¢˜: {query}")
        print("-" * 70)

        search_results = rag.search(query)

        if search_results['documents'][0]:
            print(f"ğŸ” æ£€ç´¢åˆ° {len(search_results['documents'][0])} ä¸ªç›¸å…³æ–‡æ¡£å—:")
            for metadata, distance in zip(search_results['metadatas'][0], search_results['distances'][0]):
                similarity = (1 - distance) * 100
                source = metadata.get('source', 'Unknown')
                chunk_id = metadata.get('chunk_id', '')
                chunk_info = f" [{chunk_id.split('_')[-1]}]" if chunk_id else ""
                print(f"   â€¢ {source}{chunk_info} (ç›¸ä¼¼åº¦: {similarity:.1f}%)")

            print(f"\nğŸ’¡ ç­”æ¡ˆ:")
            answer = rag.generate_answer(query)
            print(answer)
        else:
            print("ğŸ” æ²¡æœ‰æ‰¾åˆ°æ»¡è¶³ç›¸ä¼¼åº¦é˜ˆå€¼çš„ç›¸å…³æ–‡æ¡£")
            print(f"\nğŸ’¡ ç­”æ¡ˆ:")
            print("âŒ æŠ±æ­‰,åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯æ¥å›ç­”è¿™ä¸ªé—®é¢˜ã€‚")

        print("=" * 70)

    print("\nâœ¨ æ¼”ç¤ºå®Œæˆ!")
    print("ğŸ’¾ å‘é‡æ•°æ®åº“å·²æŒä¹…åŒ–ä¿å­˜,ä¸‹æ¬¡è¿è¡Œå°†è‡ªåŠ¨å¤ç”¨")


# ============ FastAPI å®ç° ============

app = FastAPI(title="RAG API", version="1.0.0")

# é…ç½® CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€ RAG å®ä¾‹
rag_instance = None

# ä¼šè¯ç®¡ç†ï¼ˆå­˜å‚¨æ¯ä¸ªä¼šè¯çš„å†å²ï¼‰
sessions: Dict[str, list] = {}


def get_rag():
    """è·å–æˆ–åˆå§‹åŒ– RAG å®ä¾‹"""
    global rag_instance
    if rag_instance is None:
        config = RAGConfig(
            max_results=10,
            similarity_threshold=0.70,
            chunk_size=1000,
            chunk_overlap=250,
            use_hybrid_search=True,
            keyword_boost=0.3
        )
        rag_instance = GeminiRAG(
            collection_name="md_documents",
            persist_directory="./chroma_db",
            config=config
        )
        rag_instance.load_documents_from_folder(
            folder_path="./docs",
            force_reload=False,
        )
    return rag_instance


# ========== Pydantic Models ==========

class QueryRequest(BaseModel):
    query: str
    session_id: Optional[str] = None
    n_results: Optional[int] = None
    similarity_threshold: Optional[float] = None


class SessionRequest(BaseModel):
    session_id: str


class ReloadRequest(BaseModel):
    force_reload: bool = False


# ========== API ç«¯ç‚¹ ==========

@app.get("/api/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "ok", "message": "RAG API is running"}


@app.get("/api/info")
async def get_info():
    """è·å–ç³»ç»Ÿä¿¡æ¯"""
    try:
        rag = get_rag()
        count = rag.collection.count()

        results = rag.collection.get()
        docs_by_source = defaultdict(list)
        for metadata in results['metadatas']:
            source = metadata.get('source', 'unknown')
            docs_by_source[source].append(metadata)

        doc_list = []
        for source, metadatas in sorted(docs_by_source.items()):
            doc_list.append({
                'source': source,
                'chunk_count': len(metadatas),
                'file_size': metadatas[0].get('file_size', 0)
            })

        return {
            'success': True,
            'total_chunks': count,
            'total_documents': len(doc_list),
            'documents': doc_list,
            'active_sessions': len(sessions),
            'config': {
                'max_results': rag.config.max_results,
                'similarity_threshold': rag.config.similarity_threshold,
                'use_hybrid_search': rag.config.use_hybrid_search,
                'keyword_boost': rag.config.keyword_boost
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/session/create")
async def create_session():
    """åˆ›å»ºæ–°çš„ä¼šè¯"""
    session_id = str(uuid.uuid4())
    sessions[session_id] = []
    return {
        'success': True,
        'session_id': session_id,
        'message': 'ä¼šè¯åˆ›å»ºæˆåŠŸ'
    }


@app.post("/api/session/clear")
async def clear_session(request: SessionRequest):
    """æ¸…ç©ºä¼šè¯å†å²"""
    session_id = request.session_id
    if session_id in sessions:
        sessions[session_id] = []
        return {
            'success': True,
            'message': 'ä¼šè¯å†å²å·²æ¸…ç©º'
        }
    else:
        raise HTTPException(status_code=404, detail='ä¼šè¯ä¸å­˜åœ¨')


@app.get("/api/session/{session_id}/history")
async def get_session_history(session_id: str):
    """è·å–ä¼šè¯å†å²"""
    if session_id in sessions:
        return {
            'success': True,
            'session_id': session_id,
            'history': sessions[session_id]
        }
    else:
        raise HTTPException(status_code=404, detail='ä¼šè¯ä¸å­˜åœ¨')


@app.post("/api/ask/stream")
async def ask_question_stream(request: QueryRequest):
    """RAG é—®ç­”æ¥å£ï¼ˆæµå¼ä¼ è¾“ï¼‰"""
    try:
        query = request.query.strip()
        if not query:
            raise HTTPException(status_code=400, detail='é—®é¢˜ä¸èƒ½ä¸ºç©º')

        session_id = request.session_id

        # å¦‚æœæ²¡æœ‰ session_idï¼Œåˆ›å»ºæ–°ä¼šè¯
        if not session_id:
            session_id = str(uuid.uuid4())
            sessions[session_id] = []
        elif session_id not in sessions:
            sessions[session_id] = []

        # è·å–å†å²è®°å½•
        chat_history = sessions[session_id]

        rag = get_rag()

        async def event_generator():
            # å‘é€ session_id
            yield f"data: {json.dumps({'type': 'session', 'session_id': session_id}, ensure_ascii=False)}\n\n"

            # æµå¼ç”Ÿæˆç­”æ¡ˆ
            full_answer = ""
            for chunk_data in rag.generate_answer_stream(
                    query,
                    chat_history=chat_history,
                    n_results=request.n_results,
                    similarity_threshold=request.similarity_threshold
            ):
                yield f"data: {chunk_data}\n\n"

                # æ”¶é›†å®Œæ•´ç­”æ¡ˆ
                chunk_obj = json.loads(chunk_data)
                if chunk_obj['type'] == 'content':
                    full_answer += chunk_obj['content']

                await asyncio.sleep(0.01)  # å°å»¶è¿Ÿï¼Œè®©æµæ›´å¹³æ»‘

            # æ›´æ–°ä¼šè¯å†å²
            sessions[session_id].append({
                'role': 'user',
                'parts': [{'text': query}]
            })
            sessions[session_id].append({
                'role': 'model',
                'parts': [{'text': full_answer}]
            })

        return StreamingResponse(
            event_generator(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no"
            }
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/search")
async def search_documents(request: QueryRequest):
    """æœç´¢ç›¸å…³æ–‡æ¡£ï¼ˆéæµå¼ï¼‰"""
    try:
        query = request.query.strip()
        if not query:
            raise HTTPException(status_code=400, detail='æŸ¥è¯¢å†…å®¹ä¸èƒ½ä¸ºç©º')

        rag = get_rag()
        search_results = rag.search(
            query,
            request.n_results,
            request.similarity_threshold
        )

        results = []
        if search_results['documents'][0]:
            for i, (doc, metadata, distance) in enumerate(zip(
                    search_results['documents'][0],
                    search_results['metadatas'][0],
                    search_results['distances'][0]
            )):
                keyword_score = search_results.get('keyword_scores', [[]])[0][
                    i] if 'keyword_scores' in search_results else 0

                results.append({
                    'document': doc,
                    'source': metadata.get('source', ''),
                    'section_path': metadata.get('section_path', ''),
                    'keywords': metadata.get('keywords', ''),
                    'chunk_id': metadata.get('chunk_id', ''),
                    'semantic_similarity': round((1 - distance) * 100, 2),
                    'keyword_score': round(keyword_score * 100, 2)
                })

        return {
            'success': True,
            'query': query,
            'results': results,
            'count': len(results)
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/reload")
async def reload_documents(request: ReloadRequest):
    """é‡æ–°åŠ è½½æ–‡æ¡£"""
    try:
        rag = get_rag()
        rag.load_documents_from_folder(
            folder_path="./docs",
            force_reload=request.force_reload
        )

        return {
            'success': True,
            'message': 'æ–‡æ¡£é‡æ–°åŠ è½½å®Œæˆ',
            'total_chunks': rag.collection.count()
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


def start_fastapi(host='0.0.0.0', port=8000):
    """å¯åŠ¨ FastAPI æœåŠ¡"""
    import uvicorn

    print("\n" + "=" * 70)
    print("ğŸš€ å¯åŠ¨ FastAPI æœåŠ¡...")
    print("=" * 70)

    print("\nğŸ“¦ é¢„åŠ è½½ RAG ç³»ç»Ÿ...")
    get_rag()

    print("\nâœ… RAG ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    print(f"ğŸŒ API æœåŠ¡è¿è¡Œåœ¨: http://{host}:{port}")
    print(f"ğŸ“š API æ–‡æ¡£: http://{host}:{port}/docs")
    print("\nğŸ“¡ å¯ç”¨çš„ API ç«¯ç‚¹:")
    print(f"   â€¢ GET  /api/health              - å¥åº·æ£€æŸ¥")
    print(f"   â€¢ GET  /api/info                - ç³»ç»Ÿä¿¡æ¯")
    print(f"   â€¢ POST /api/session/create      - åˆ›å»ºä¼šè¯")
    print(f"   â€¢ POST /api/session/clear       - æ¸…ç©ºä¼šè¯")
    print(f"   â€¢ GET  /api/session/:id/history - è·å–å†å²")
    print(f"   â€¢ POST /api/ask/stream          - RAG é—®ç­”ï¼ˆæµå¼ï¼‰")
    print(f"   â€¢ POST /api/search              - æœç´¢æ–‡æ¡£")
    print(f"   â€¢ POST /api/reload              - é‡æ–°åŠ è½½æ–‡æ¡£")
    print("=" * 70 + "\n")

    uvicorn.run(app, host=host, port=port)


if __name__ == "__main__":
    import sys

    start_fastapi(host='0.0.0.0', port=8000)
    #if len(sys.argv) > 1 and sys.argv[1] == 'api':
        #start_fastapi(host='0.0.0.0', port=8000)
    #else:
        # åŸæ¥çš„æµ‹è¯•ä»£ç 
        #main()