"""
æ”¹è¿›çš„ Gemini RAG ç³»ç»Ÿ - æ··åˆæ£€ç´¢ç­–ç•¥
æ ¸å¿ƒæ”¹è¿›: å…³é”®è¯åŒ¹é… + è¯­ä¹‰æ£€ç´¢ åŒé‡ä¿éšœ
ä¸“é—¨ä¼˜åŒ–äººåã€ä¸“æœ‰åè¯çš„æ£€ç´¢å‡†ç¡®åº¦
"""
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import Optional, Dict
import asyncio
import json
import uuid
from datetime import datetime

from flask import Flask, request, jsonify
from flask_cors import CORS
import hashlib
import os
from pathlib import Path
from typing import List, Optional, Tuple, Dict, Set
from sentence_transformers import SentenceTransformer
import chromadb
import google.generativeai as genai
import re
from collections import defaultdict

from starlette.staticfiles import StaticFiles

from implements.RAGConfig import RAGConfig

API_KEY = 'AIzaSyCNwmo17IETTpEAhCp9mvrtaovXteITZDM'

genai.configure(api_key=API_KEY)
os.environ["http_proxy"] = "http://127.0.0.1:10809"
os.environ["https_proxy"] = "http://127.0.0.1:10809"


# è¯·å°†ä¸‹é¢çš„æ•´ä¸ª class GeminiRAG æ›¿æ¢æ‰æ‚¨æ–‡ä»¶ä¸­çš„åŒå class

class GeminiRAG:
    """
    ä¸€ä¸ªé›†æˆäº†æŸ¥è¯¢é‡å†™ã€å¢å¼ºåµŒå…¥ã€ä¸Šä¸‹æ–‡çª—å£æ‰©å±•å’Œå¤šè½®å¯¹è¯å†å²çš„
    é«˜çº§æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) ç³»ç»Ÿã€‚
    """

    def __init__(
            self,
            collection_name: str = "documents_optimized",
            persist_directory: str = "./chroma_db_optimized",
            config: Optional[RAGConfig] = None
    ):
        """åˆå§‹åŒ– RAG ç³»ç»Ÿ"""
        self.persist_directory = persist_directory
        self.config = config or RAGConfig()

        print(f"ğŸ“¦ åˆå§‹åŒ– ChromaDB (å­˜å‚¨è·¯å¾„: {persist_directory})...")
        self.client = chromadb.PersistentClient(path=persist_directory)

        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            metadata={"hnsw:space": "cosine"}
        )

        existing_count = self.collection.count()
        if existing_count > 0:
            print(f"âœ… å‘ç°å·²å­˜åœ¨çš„æ•°æ®åº“,åŒ…å« {existing_count} ä¸ªæ–‡æ¡£å—")

        print("ğŸ“¦ åŠ è½½åµŒå…¥æ¨¡å‹ (paraphrase-multilingual-MiniLM-L12-v2)...")
        self.embed_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        print("âœ… åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆ")

        self.chat_model = genai.GenerativeModel('gemini-1.5-flash')

        print(f"\nâš™ï¸  RAG ç³»ç»Ÿé…ç½®:")
        print(f"   â”œâ”€ æ£€ç´¢ç­–ç•¥: {'æ··åˆæ£€ç´¢ (å…³é”®è¯+è¯­ä¹‰)' if self.config.use_hybrid_search else 'çº¯è¯­ä¹‰æ£€ç´¢'}")
        if self.config.use_hybrid_search:
            print(f"   â”‚  â””â”€ å…³é”®è¯æƒé‡: {self.config.keyword_boost}")
        print(f"   â”œâ”€ æŸ¥è¯¢é‡å†™ä¼˜åŒ–: {'âœ… å·²å¯ç”¨' if self.config.use_query_rewriting else 'âŒ æœªå¯ç”¨'}")
        print(
            f"   â”œâ”€ ä¸Šä¸‹æ–‡çª—å£æ‰©å±•: {'âœ… å·²å¯ç”¨ (çª—å£å¤§å°: ' + str(self.config.context_window_size) + ')' if self.config.context_window_size > 1 else 'âŒ æœªå¯ç”¨'}")
        print(f"   â”œâ”€ æœ€å¤§æ£€ç´¢æ•°: {self.config.max_results}")
        print(f"   â”œâ”€ ç›¸ä¼¼åº¦é˜ˆå€¼: {self.config.similarity_threshold or 'è‡ªåŠ¨'}")
        print(f"   â”œâ”€ åˆ†å—å¤§å°: {self.config.chunk_size} å­—ç¬¦")
        print(f"   â””â”€ é‡å å¤§å°: {self.config.chunk_overlap} å­—ç¬¦")

    # ... [ä» _get_embedding åˆ° _smart_chunk_document çš„æ‰€æœ‰è¾…åŠ©å‡½æ•°ä¿æŒä¸å˜] ...
    def _get_embedding(self, text: str) -> List[float]:
        """ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡"""
        embedding = self.embed_model.encode(text, convert_to_tensor=False)
        return embedding.tolist()

    def _get_file_hash(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶çš„ MD5 å“ˆå¸Œå€¼"""
        with open(file_path, 'rb') as f:
            return hashlib.md5(f.read()).hexdigest()

    def _read_markdown_file(self, file_path: Path) -> str:
        """è¯»å– Markdown æ–‡ä»¶å†…å®¹"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            print(f"âš ï¸  è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return None

    def _extract_keywords(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯"""
        keywords = []
        hashtags = re.findall(r'#([^\s#]+)', text)
        keywords.extend(hashtags)
        bold_text = re.findall(r'\*\*([^*]+)\*\*', text)
        keywords.extend([t.strip() for t in bold_text if 3 < len(t.strip()) < 30])
        headers = re.findall(r'^#{1,6}\s+(.+)$', text, re.MULTILINE)
        keywords.extend([h.strip() for h in headers if len(h.strip()) < 50])
        names = re.findall(r'([Â·\u4e00-\u9fa5]{2,6}(?:çš„|ã€|ä¸|å’Œ|åŠ)?)', text)
        potential_names = [n.strip('çš„ã€ä¸å’ŒåŠ') for n in names if 2 <= len(n.strip('çš„ã€ä¸å’ŒåŠ')) <= 6]
        keywords.extend(potential_names)
        return list(set(keywords))

    def _extract_query_keywords(self, query: str) -> List[str]:
        """ä»æŸ¥è¯¢ä¸­æå–å…³é”®è¯ï¼Œç”¨äºæ··åˆæ£€ç´¢è¯„åˆ†"""
        keywords = set(re.findall(r'[\u4e00-\u9fa5Â·]{2,6}', query))
        key_phrases = ['æ”¿æ²»æ€æƒ³', 'ç†è®º', 'è§‚ç‚¹', 'å­¦è¯´', 'ä¸»å¼ ', 'æ‰¹åˆ¤', 'è¯„ä»·', 'æ­£ä¹‰è®º']
        for phrase in key_phrases:
            if phrase in query:
                keywords.add(phrase)
        return list(keywords)

    def _keyword_match_score(self, query: str, doc_text: str, doc_keywords: str) -> float:
        """è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°"""
        score = 0.0
        query_keywords = self._extract_query_keywords(query)
        if not query_keywords:
            return 0.0
        doc_text_lower = doc_text.lower()
        doc_keywords_lower = doc_keywords.lower() if doc_keywords else ""
        for keyword in query_keywords:
            keyword_lower = keyword.lower()
            if keyword_lower in doc_keywords_lower:
                score += 0.5
            count = min(doc_text_lower.count(keyword_lower), 5)
            score += count * 0.1
        return min(score, 1.0)

    def _parse_document_structure(self, text: str) -> List[Dict]:
        """è§£ææ–‡æ¡£ç»“æ„ï¼Œè¯†åˆ«æ ‡é¢˜å’Œç« èŠ‚"""
        lines = text.split('\n')
        sections = []
        current_section = {'content': [], 'headers': [], 'line_start': 0, 'header_level': 0}
        header_stack = []
        for i, line in enumerate(lines):
            header_match = re.match(r'^(#{1,6})\s+(.+)$', line.strip())
            if header_match:
                if current_section['content']:
                    current_section['content'] = '\n'.join(current_section['content'])
                    sections.append(current_section)
                level, title = len(header_match.group(1)), header_match.group(2).strip()
                while header_stack and header_stack[-1]['level'] >= level:
                    header_stack.pop()
                header_stack.append({'level': level, 'title': title})
                current_section = {
                    'content': [line], 'headers': [h['title'] for h in header_stack],
                    'header_level': level, 'line_start': i
                }
            else:
                current_section['content'].append(line)
        if current_section['content']:
            current_section['content'] = '\n'.join(current_section['content'])
            sections.append(current_section)
        return sections

    def _split_long_text(self, text: str, max_size: int) -> List[str]:
        """å½“å•ä¸ªç« èŠ‚å†…å®¹è¿‡é•¿æ—¶ï¼ŒæŒ‰å¥å­è¿›è¡Œåˆ†å‰²"""
        if len(text) <= max_size:
            return [text]
        sentences = re.split(r'([ã€‚ï¼ï¼Ÿ\n]+)', text)
        parts = []
        current_part = ""
        for i in range(0, len(sentences), 2):
            sentence = sentences[i]
            delimiter = sentences[i + 1] if i + 1 < len(sentences) else ""
            if len(current_part) + len(sentence) + len(delimiter) > max_size and current_part:
                parts.append(current_part)
                overlap_start = max(0, len(current_part) - self.config.chunk_overlap)
                current_part = current_part[overlap_start:] + sentence + delimiter
            else:
                current_part += sentence + delimiter
        if current_part:
            parts.append(current_part)
        return parts if parts else [text[:max_size]]

    def _smart_chunk_document(self, text: str, file_name: str) -> List[tuple]:
        """æ™ºèƒ½åˆ†å—æ–‡æ¡£ï¼šåŸºäºMarkdownç»“æ„ï¼Œåˆå¹¶å°ç« èŠ‚ï¼Œåˆ†å‰²å¤§ç« èŠ‚"""
        sections = self._parse_document_structure(text)
        chunks = []
        chunk_idx = 0
        i = 0
        while i < len(sections):
            headers_text = ' > '.join(sections[i].get('headers', []))
            chunk_content_parts = []
            current_size = 0
            start_section_idx = i
            j = i
            while j < len(sections):
                section_to_add = sections[j]
                content_to_add = section_to_add['content']
                if j > start_section_idx and section_to_add.get('header_level', 6) <= 2:
                    break
                if current_size > 0 and current_size + len(content_to_add) > self.config.chunk_size:
                    break
                chunk_content_parts.append(content_to_add)
                current_size += len(content_to_add)
                j += 1
            full_content = "\n\n".join(chunk_content_parts)
            split_parts = self._split_long_text(full_content, self.config.chunk_size) if len(
                full_content) > self.config.chunk_size else [full_content]
            for part in split_parts:
                keywords = self._extract_keywords(f"# {headers_text}\n{part}")
                chunk_meta = {'section_path': headers_text, 'keywords': ', '.join(keywords)}
                chunk_id = f"{file_name}_chunk_{chunk_idx}"
                chunks.append((part, chunk_id, len(part), chunk_meta))
                chunk_idx += 1
            i = j - 1 if j > start_section_idx + 1 else j
        return chunks

    def load_documents_from_folder(self, folder_path: str = "./docs", force_reload: bool = False):
        """ä»æ–‡ä»¶å¤¹é€’å½’åŠ è½½æ‰€æœ‰ Markdown æ–‡æ¡£ (åŒ…å«åµŒå…¥ä¼˜åŒ–å’Œè·¯å¾„ä¿®å¤)"""
        # [FIX] ä½¿ç”¨ resolve() è·å–ç»å¯¹è·¯å¾„ï¼Œç¡®ä¿è·¯å¾„ä¸€è‡´æ€§
        docs_path = Path(folder_path).resolve()
        if not docs_path.exists():
            print(f"âŒ æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {docs_path}")
            return

        md_files = list(docs_path.rglob("*.md"))
        if not md_files:
            print(f"âš ï¸  åœ¨ {docs_path} ä¸­æ²¡æœ‰æ‰¾åˆ° Markdown æ–‡ä»¶ã€‚")
            return

        print(f"\nğŸ“ æ‰¾åˆ° {len(md_files)} ä¸ª Markdown æ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")

        total_docs_in_db = self.collection.count()
        all_metadatas = self.collection.get(limit=total_docs_in_db, include=["metadatas"])[
            'metadatas'] if total_docs_in_db > 0 else []
        existing_hashes = {meta.get('source'): meta.get('file_hash') for meta in all_metadatas if
                           'source' in meta and 'file_hash' in meta}

        new_docs_content, new_docs_for_embedding, new_ids, new_metadatas = [], [], [], []
        ids_to_delete = []
        updated_count, new_count, skipped_count = 0, 0, 0

        for md_file in md_files:
            # [FIX] ç¡®ä¿ relative_path ä½¿ç”¨ç»å¯¹è·¯å¾„ä½œä¸ºåŸºå‡†
            relative_path = str(md_file.relative_to(docs_path)).replace('\\', '/')
            current_hash = self._get_file_hash(str(md_file))

            if relative_path in existing_hashes:
                if not force_reload and existing_hashes[relative_path] == current_hash:
                    skipped_count += 1
                    continue
                else:
                    print(f"ğŸ”„ æ£€æµ‹åˆ°æ–‡ä»¶å˜æ›´: {relative_path}")
                    updated_count += 1
                    results = self.collection.get(where={"source": relative_path}, include=[])
                    ids_to_delete.extend(results['ids'])
            else:
                new_count += 1

            content = self._read_markdown_file(md_file)
            if content:
                chunks = self._smart_chunk_document(content, relative_path)
                for chunk_text, chunk_id, chunk_size, chunk_meta in chunks:
                    new_docs_content.append(chunk_text)
                    new_ids.append(chunk_id)
                    embedding_text = f"æ‰€å±ç« èŠ‚: {chunk_meta.get('section_path', '')}\nå…³é”®è¯: {chunk_meta.get('keywords', '')}\n\nå†…å®¹:\n{chunk_text}"
                    new_docs_for_embedding.append(embedding_text)
                    metadata = {"source": relative_path, "file_hash": current_hash, **chunk_meta}
                    new_metadatas.append(metadata)

        if ids_to_delete:
            print(f"ğŸ—‘ï¸  æ­£åœ¨åˆ é™¤ {len(ids_to_delete)} ä¸ªæ—§çš„æ–‡æ¡£å—...")
            delete_batch_size = 500
            for i in range(0, len(ids_to_delete), delete_batch_size):
                self.collection.delete(ids=ids_to_delete[i:i + delete_batch_size])
            print("âœ… æ—§æ–‡æ¡£å—åˆ é™¤å®Œæ¯•ã€‚")

        if new_docs_content:
            print(
                f"\nğŸ’¾ å‡†å¤‡å¤„ç† {new_count} ä¸ªæ–°æ–‡ä»¶å’Œ {updated_count} ä¸ªæ›´æ–°æ–‡ä»¶ï¼Œå…±è®¡ {len(new_docs_content)} ä¸ªæ–°æ–‡æ¡£å—...")
            print("ğŸ”„ ç”Ÿæˆå¢å¼ºåµŒå…¥å‘é‡...")
            embedding_batch_size = 32
            embeddings = []
            for i in range(0, len(new_docs_for_embedding), embedding_batch_size):
                batch = new_docs_for_embedding[i:i + embedding_batch_size]
                batch_embeddings = self.embed_model.encode(batch, convert_to_tensor=False,
                                                           show_progress_bar=False).tolist()
                embeddings.extend(batch_embeddings)
                print(
                    f"   ç”ŸæˆåµŒå…¥å‘é‡è¿›åº¦: {min(i + embedding_batch_size, len(new_docs_for_embedding))}/{len(new_docs_for_embedding)}")

            db_batch_size = 4000
            total_batches = (len(new_ids) + db_batch_size - 1) // db_batch_size
            print(f"\nâ• æ­£åœ¨å°† {len(new_ids)} ä¸ªæ–‡æ¡£å—åˆ† {total_batches} æ‰¹æ·»åŠ åˆ°æ•°æ®åº“...")
            for i in range(0, len(new_ids), db_batch_size):
                self.collection.add(
                    ids=new_ids[i:i + db_batch_size],
                    documents=new_docs_content[i:i + db_batch_size],
                    embeddings=embeddings[i:i + db_batch_size],
                    metadatas=new_metadatas[i:i + db_batch_size]
                )
                print(f"   æ‰¹æ¬¡ {i // db_batch_size + 1}/{total_batches} æ·»åŠ æˆåŠŸã€‚")
            print(f"âœ… æˆåŠŸæ·»åŠ /æ›´æ–° {len(new_docs_content)} ä¸ªæ–‡æ¡£å—ã€‚")

        if skipped_count > 0:
            print(f"â­ï¸  è·³è¿‡ {skipped_count} ä¸ªæœªä¿®æ”¹çš„æ–‡æ¡£ã€‚")
        print(f"\nğŸ“Š æ•°æ®åº“å½“å‰å…±æœ‰ {self.collection.count()} ä¸ªæ–‡æ¡£å—ã€‚")

    def _rewrite_query_for_retrieval(self, query: str) -> str:
        if not self.config.use_query_rewriting:
            return query
        print(f"\nğŸ”„ æ­£åœ¨é‡å†™æŸ¥è¯¢...")
        prompt = f"""ä½ æ˜¯ä¸€åæ£€ç´¢ä¼˜åŒ–ä¸“å®¶ã€‚è¯·å°†ä»¥ä¸‹ç”¨æˆ·é—®é¢˜æ”¹å†™ä¸ºä¸€ä¸ªä¿¡æ¯æ›´ä¸°å¯Œçš„é™ˆè¿°å¥ï¼Œç”¨äºå‘é‡æ•°æ®åº“çš„è¯­ä¹‰æ£€ç´¢ã€‚è¯·ä¸“æ³¨äºæ ¸å¿ƒæ„å›¾ï¼Œè¡¥å……å¯èƒ½çš„ä¸Šä¸‹æ–‡ï¼Œä½¿å…¶æ›´åƒä¸€ä¸ªâ€œç­”æ¡ˆâ€çš„ç‰‡æ®µã€‚ç›´æ¥è¿”å›æ”¹å†™åçš„æ–‡æœ¬ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæˆ–å‰ç¼€ã€‚åŸå§‹é—®é¢˜: "{query}"\n\næ”¹å†™åçš„æ£€ç´¢æŸ¥è¯¢:"""
        try:
            response = self.chat_model.generate_content(prompt)
            rewritten_query = response.text.strip().replace("*", "")
            print(f"   - åŸå§‹æŸ¥è¯¢: {query}")
            print(f"   - é‡å†™å: {rewritten_query}")
            return rewritten_query
        except Exception as e:
            print(f"âš ï¸ æŸ¥è¯¢é‡å†™å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨åŸå§‹æŸ¥è¯¢ã€‚")
            return query

    def search(self, query: str) -> dict:
        rewritten_query = self._rewrite_query_for_retrieval(query)
        query_embedding = self._get_embedding(rewritten_query)
        search_n = min(self.config.max_results * 5, self.collection.count())
        if search_n == 0:
            return {'ids': [[]], 'documents': [[]], 'metadatas': [[]], 'distances': [[]]}

        semantic_results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=search_n,
            include=["metadatas", "documents", "distances"]
        )
        if not semantic_results['documents'][0]:
            return semantic_results

        if self.config.use_hybrid_search:
            scored_results = []
            for doc_id, doc, meta, dist in zip(semantic_results['ids'][0], semantic_results['documents'][0],
                                               semantic_results['metadatas'][0], semantic_results['distances'][0]):
                semantic_score = 1 - dist
                keyword_score = self._keyword_match_score(query, doc, meta.get('keywords', ''))
                final_score = semantic_score * (
                            1 - self.config.keyword_boost) + keyword_score * self.config.keyword_boost
                scored_results.append({'id': doc_id, 'doc': doc, 'meta': meta, 'dist': dist, 'score': final_score})
            scored_results.sort(key=lambda x: x['score'], reverse=True)
            filtered = [r for r in scored_results if (1 - r['dist']) >= self.config.similarity_threshold]
            top_results = filtered[:self.config.max_results]
            return {
                'ids': [[r['id'] for r in top_results]], 'documents': [[r['doc'] for r in top_results]],
                'metadatas': [[r['meta'] for r in top_results]], 'distances': [[r['dist'] for r in top_results]]
            }

        indices = [i for i, d in enumerate(semantic_results['distances'][0]) if
                   (1 - d) >= self.config.similarity_threshold]
        top_indices = indices[:self.config.max_results]
        return {
            'ids': [[semantic_results['ids'][0][i] for i in top_indices]],
            'documents': [[semantic_results['documents'][0][i] for i in top_indices]],
            'metadatas': [[semantic_results['metadatas'][0][i] for i in top_indices]],
            'distances': [[semantic_results['distances'][0][i] for i in top_indices]]
        }

    def _expand_context_with_window(self, search_results: dict) -> List[Dict]:
        if self.config.context_window_size <= 1 or not search_results['ids'][0]:
            return [{"doc": doc, "meta": meta, "is_hit": True} for doc, meta in
                    zip(search_results['documents'][0], search_results['metadatas'][0])]

        print("ğŸ”„ æ­£åœ¨æ‰©å±•ä¸Šä¸‹æ–‡çª—å£...")
        final_docs = {r_id: {"doc": doc, "meta": meta, "is_hit": True} for r_id, doc, meta in
                      zip(search_results['ids'][0], search_results['documents'][0], search_results['metadatas'][0])}
        ids_to_fetch = set()
        window_radius = self.config.context_window_size // 2
        for r_id in search_results['ids'][0]:
            parts = r_id.rsplit('_chunk_', 1)
            if len(parts) == 2 and parts[1].isdigit():
                base_name, index = parts[0], int(parts[1])
                for i in range(1, window_radius + 1):
                    prev_id = f"{base_name}_chunk_{index - i}"
                    next_id = f"{base_name}_chunk_{index + i}"
                    if prev_id not in final_docs: ids_to_fetch.add(prev_id)
                    if next_id not in final_docs: ids_to_fetch.add(next_id)
        if ids_to_fetch:
            ids_list = list(ids_to_fetch)
            get_batch_size = 500
            for i in range(0, len(ids_list), get_batch_size):
                batch_ids = ids_list[i:i + get_batch_size]
                context_docs = self.collection.get(ids=batch_ids, include=["metadatas", "documents"])
                for c_id, doc, meta in zip(context_docs['ids'], context_docs['documents'], context_docs['metadatas']):
                    if c_id not in final_docs:
                        final_docs[c_id] = {"doc": doc, "meta": meta, "is_hit": False}
        sorted_ids = sorted(final_docs.keys(),
                            key=lambda x: (final_docs[x]['meta']['source'], int(x.rsplit('_', 1)[1])))
        return [final_docs[id] for id in sorted_ids]

    # ============== [æ ¸å¿ƒä¿®æ”¹åŒºåŸŸ] ==============
    # ç”¨ä¸‹é¢è¿™ä¸ªå®ç°äº†å¤šè½®å¯¹è¯çš„ç‰ˆæœ¬ï¼Œæ›¿æ¢æ‚¨å½“å‰çš„ generate_answer_stream å’Œ generate_answer æ–¹æ³•
    # ============================================

    def generate_answer_stream(self, query: str, chat_history: list = None):
        """
        RAG æµå¼ç”Ÿæˆç­”æ¡ˆï¼ˆæ”¯æŒå¤šè½®å¯¹è¯ï¼‰ã€‚
        æ­¤æ–¹æ³•ä¼šå…ˆæ£€ç´¢ä¸å½“å‰é—®é¢˜ç›¸å…³çš„æ–‡æ¡£ï¼Œç„¶åå°†è¿™äº›æ–‡æ¡£ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œ
        è¿åŒå†å²å¯¹è¯è®°å½•ä¸€èµ·ï¼Œäº¤ç»™å¤§æ¨¡å‹ç”Ÿæˆå›ç­”ã€‚
        """
        # æ­¥éª¤ 1: æ£€ç´¢ä¸å½“å‰é—®é¢˜ç›¸å…³çš„æ–‡æ¡£
        search_results = self.search(query)
        has_results = (search_results and search_results.get('documents') and search_results['documents'][0])

        if not has_results:
            yield json.dumps({'type': 'error', 'content': 'æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£,æ— æ³•å›ç­”é—®é¢˜ã€‚'}, ensure_ascii=False) + '\n'
            return

        # æ­¥éª¤ 2: æ‰©å±•ä¸Šä¸‹æ–‡çª—å£ï¼Œè·å–æ›´å®Œæ•´çš„æ–‡æ¡£ç‰‡æ®µ
        context_items = self._expand_context_with_window(search_results)

        # æ­¥éª¤ 3: å‡†å¤‡å¹¶å‘é€ "sources" ä¿¡æ¯
        sources_info = [{'source': item['meta']['source'], 'section_path': item['meta'].get('section_path', ''),
                         'is_hit': item['is_hit']} for item in context_items]
        yield json.dumps({'type': 'sources', 'content': sources_info, 'count': len(sources_info)},
                         ensure_ascii=False) + '\n'

        # æ­¥éª¤ 4: æ„å»ºä»…åŒ…å«å½“å‰æ£€ç´¢åˆ°çš„æ–‡æ¡£çš„ä¸Šä¸‹æ–‡æ–‡æœ¬
        context_parts = []
        for item in context_items:
            source_info = f"[æ¥æº: {item['meta'].get('source', 'æœªçŸ¥')} | ç« èŠ‚: {item['meta'].get('section_path', 'N/A')}]"
            context_parts.append(f"{source_info}\n{item['doc']}")
        context_text = "\n\n---\n\n".join(context_parts)
        print(f"\nğŸ“š æœ¬è½®æ£€ç´¢åˆ° {len(context_parts)} ä¸ªæ–‡æ¡£å—ä½œä¸ºä¸Šä¸‹æ–‡ã€‚")

        # æ­¥éª¤ 5: åˆ›å»ºä¸€ä¸ªæœ‰çŠ¶æ€çš„å¯¹è¯å®ä¾‹ï¼Œå¹¶è½½å…¥å†å²è®°å½•
        chat = self.chat_model.start_chat(history=chat_history or [])

        # æ­¥éª¤ 6: æ„å»ºå‘é€ç»™æ¨¡å‹çš„æœ€ç»ˆæ¶ˆæ¯
        # è¿™æ¡æ¶ˆæ¯åŒ…å«äº†ç³»ç»ŸæŒ‡ä»¤ã€æœ¬è½®æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡å’Œå½“å‰ç”¨æˆ·çš„é—®é¢˜
        user_message = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ”¿æ²»å­¦çŸ¥è¯†è§£ç­”æ¨¡å‹ã€‚è¯·ä¸¥æ ¼åŸºäºä»¥ä¸‹æœ€æ–°æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ï¼Œå¹¶ç»“åˆä¹‹å‰çš„å¯¹è¯å†å²ï¼Œä»¥ç³»ç»Ÿã€å­¦æœ¯åŒ–çš„æ–¹å¼å›ç­”ç”¨æˆ·å½“å‰çš„é—®é¢˜ã€‚ä»¥Markdownæ ¼å¼è¿›è¡Œå›å¤ã€‚

--- [æœ€æ–°æ£€ç´¢åˆ°çš„æ–‡æ¡£ (ç”¨äºå›ç­”å½“å‰é—®é¢˜)] ---
{context_text}
--- [æ£€ç´¢åˆ°çš„æ–‡æ¡£ç»“æŸ] ---

å½“å‰é—®é¢˜: {query}
"""

        # æ­¥éª¤ 7: æµå¼å‘é€æ¶ˆæ¯å¹¶è¿”å›ç»“æœ
        try:
            response = chat.send_message(user_message, stream=True)
            for chunk in response:
                if chunk.text:
                    yield json.dumps({'type': 'content', 'content': chunk.text}, ensure_ascii=False) + '\n'
            yield json.dumps({'type': 'done', 'content': ''}, ensure_ascii=False) + '\n'
        except Exception as e:
            yield json.dumps({'type': 'error', 'content': f'ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {str(e)}'}, ensure_ascii=False) + '\n'

    def generate_answer(self, query: str) -> str:
        """RAGéæµå¼ç”Ÿæˆç­”æ¡ˆï¼ˆä¸ºä¿æŒå…¼å®¹æ€§è€Œä¿ç•™ï¼Œä½†ä¸æ”¯æŒå¤šè½®å¯¹è¯ï¼‰"""
        full_response = ""
        # éæµå¼æ–¹æ³•æœ¬è´¨ä¸Šæ˜¯æµå¼æ–¹æ³•çš„èšåˆ
        for chunk_data in self.generate_answer_stream(query, chat_history=None):
            chunk = json.loads(chunk_data)
            if chunk['type'] == 'content':
                full_response += chunk['content']
            elif chunk['type'] == 'error':
                return f"âŒ {chunk['content']}"
        return full_response

    def get_collection_info(self):
        """è·å–é›†åˆä¿¡æ¯"""
        count = self.collection.count()
        return f"é›†åˆä¸­å…±æœ‰ {count} ä¸ªæ–‡æ¡£å—"

    def list_documents(self, show_sample: bool = False):
        """åˆ—å‡ºæ‰€æœ‰æ–‡æ¡£"""
        results = self.collection.get()
        print(f"\nğŸ“š æ•°æ®åº“ä¸­çš„æ–‡æ¡£åˆ—è¡¨:")
        print("=" * 80)

        docs_by_source = defaultdict(list)
        for doc_id, metadata in zip(results['ids'], results['metadatas']):
            source = metadata.get('source', doc_id)
            docs_by_source[source].append(metadata)

        for i, (source, metadatas) in enumerate(sorted(docs_by_source.items()), 1):
            file_size = metadatas[0].get('file_size', 0)
            chunk_count = len(metadatas)
            total_chunk_size = sum(m.get('chunk_size', 0) for m in metadatas)
            avg_chunk_size = total_chunk_size // chunk_count if chunk_count > 0 else 0

            print(f"\n{i}. {source}")
            print(f"   å¤§å°: {file_size:,} bytes â†’ {chunk_count} å— (å¹³å‡ {avg_chunk_size} å­—ç¬¦/å—)")

            if show_sample and i <= 3:
                # æ˜¾ç¤ºå‰3ä¸ªå—çš„å…³é”®è¯
                for j, meta in enumerate(metadatas[:3]):
                    keywords = meta.get('keywords', '')
                    if keywords:
                        kw_list = keywords.split(', ')[:8]
                        print(f"   å—{j}: {', '.join(kw_list)}")

        print("=" * 80)


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ åˆå§‹åŒ–æ··åˆæ£€ç´¢ RAG ç³»ç»Ÿ...")

    config = RAGConfig(
        max_results=10,
        similarity_threshold=0.70,  # å¯é€‚å½“è°ƒæ•´
        chunk_size=1000,
        chunk_overlap=250,
        use_hybrid_search=True,
        keyword_boost=0.3  # 30%æƒé‡ç»™å…³é”®è¯åŒ¹é…
    )
    rag = GeminiRAG(
        collection_name="md_documents",
        persist_directory="./chroma_db",
        config=config
    )

    print("\nğŸ“– ä» docs æ–‡ä»¶å¤¹åŠ è½½æ–‡æ¡£...")
    rag.load_documents_from_folder(
        folder_path="./docs",
        force_reload=False,
    )

    rag.list_documents()
    print(f"\nğŸ“Š {rag.get_collection_info()}")

    print("\n" + "=" * 70)
    print("ğŸ¤– å¼€å§‹æµ‹è¯•æŸ¥è¯¢...")
    print("=" * 70)

    queries = [
        "å¥¥å…‹è‚–ç‰¹çš„æ”¿æ²»æ€æƒ³",
        "å¦‚ä½•è¯„ä»·ç½—å°”æ–¯",
        "æ ¼æ—çš„æ”¿æ²»æ€æƒ³",
        "è¯ºé½å…‹çš„ç†è®ºä»‹ç»",
    ]

    for query in queries:
        print(f"\nâ“ é—®é¢˜: {query}")
        print("-" * 70)

        search_results = rag.search(query)

        if search_results['documents'][0]:
            print(f"ğŸ” æ£€ç´¢åˆ° {len(search_results['documents'][0])} ä¸ªç›¸å…³æ–‡æ¡£å—:")
            for metadata, distance in zip(search_results['metadatas'][0], search_results['distances'][0]):
                similarity = (1 - distance) * 100
                source = metadata.get('source', 'Unknown')
                chunk_id = metadata.get('chunk_id', '')
                chunk_info = f" [{chunk_id.split('_')[-1]}]" if chunk_id else ""
                print(f"   â€¢ {source}{chunk_info} (ç›¸ä¼¼åº¦: {similarity:.1f}%)")

            print(f"\nğŸ’¡ ç­”æ¡ˆ:")
            answer = rag.generate_answer(query)
            print(answer)
        else:
            print("ğŸ” æ²¡æœ‰æ‰¾åˆ°æ»¡è¶³ç›¸ä¼¼åº¦é˜ˆå€¼çš„ç›¸å…³æ–‡æ¡£")
            print(f"\nğŸ’¡ ç­”æ¡ˆ:")
            print("âŒ æŠ±æ­‰,åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯æ¥å›ç­”è¿™ä¸ªé—®é¢˜ã€‚")

        print("=" * 70)

    print("\nâœ¨ æ¼”ç¤ºå®Œæˆ!")
    print("ğŸ’¾ å‘é‡æ•°æ®åº“å·²æŒä¹…åŒ–ä¿å­˜,ä¸‹æ¬¡è¿è¡Œå°†è‡ªåŠ¨å¤ç”¨")


# ============ FastAPI å®ç° ============

app = FastAPI(title="RAG API", version="1.0.0")

# é…ç½® CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€ RAG å®ä¾‹
rag_instance = None

# ä¼šè¯ç®¡ç†ï¼ˆå­˜å‚¨æ¯ä¸ªä¼šè¯çš„å†å²ï¼‰
sessions: Dict[str, list] = {}


def get_rag():
    """è·å–æˆ–åˆå§‹åŒ– RAG å®ä¾‹"""
    global rag_instance
    if rag_instance is None:
        config = RAGConfig(
            max_results=10,
            similarity_threshold=0.70,
            chunk_size=1000,
            chunk_overlap=250,
            use_hybrid_search=True,
            keyword_boost=0.3
        )
        rag_instance = GeminiRAG(
            collection_name="md_documents",
            persist_directory="./chroma_db",
            config=config
        )
        rag_instance.load_documents_from_folder(
            folder_path="./docs",
            force_reload=False,
        )
    return rag_instance


# ========== Pydantic Models ==========

class QueryRequest(BaseModel):
    query: str
    session_id: Optional[str] = None
    n_results: Optional[int] = None
    similarity_threshold: Optional[float] = None


class SessionRequest(BaseModel):
    session_id: str


class ReloadRequest(BaseModel):
    force_reload: bool = False


# ========== API ç«¯ç‚¹ ==========

@app.get("/api/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "ok", "message": "RAG API is running"}


@app.get("/api/info")
async def get_info():
    """è·å–ç³»ç»Ÿä¿¡æ¯"""
    try:
        rag = get_rag()
        count = rag.collection.count()

        results = rag.collection.get()
        docs_by_source = defaultdict(list)
        for metadata in results['metadatas']:
            source = metadata.get('source', 'unknown')
            docs_by_source[source].append(metadata)

        doc_list = []
        for source, metadatas in sorted(docs_by_source.items()):
            doc_list.append({
                'source': source,
                'chunk_count': len(metadatas),
                'file_size': metadatas[0].get('file_size', 0)
            })

        return {
            'success': True,
            'total_chunks': count,
            'total_documents': len(doc_list),
            'documents': doc_list,
            'active_sessions': len(sessions),
            'config': {
                'max_results': rag.config.max_results,
                'similarity_threshold': rag.config.similarity_threshold,
                'use_hybrid_search': rag.config.use_hybrid_search,
                'keyword_boost': rag.config.keyword_boost
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/session/create")
async def create_session():
    """åˆ›å»ºæ–°çš„ä¼šè¯"""
    session_id = str(uuid.uuid4())
    sessions[session_id] = []
    return {
        'success': True,
        'session_id': session_id,
        'message': 'ä¼šè¯åˆ›å»ºæˆåŠŸ'
    }


@app.post("/api/session/clear")
async def clear_session(request: SessionRequest):
    """æ¸…ç©ºä¼šè¯å†å²"""
    session_id = request.session_id
    if session_id in sessions:
        sessions[session_id] = []
        return {
            'success': True,
            'message': 'ä¼šè¯å†å²å·²æ¸…ç©º'
        }
    else:
        raise HTTPException(status_code=404, detail='ä¼šè¯ä¸å­˜åœ¨')


@app.get("/api/session/{session_id}/history")
async def get_session_history(session_id: str):
    """è·å–ä¼šè¯å†å²"""
    if session_id in sessions:
        return {
            'success': True,
            'session_id': session_id,
            'history': sessions[session_id]
        }
    else:
        raise HTTPException(status_code=404, detail='ä¼šè¯ä¸å­˜åœ¨')


@app.post("/api/ask/stream")
async def ask_question_stream(request: QueryRequest):
    """RAG é—®ç­”æ¥å£ï¼ˆæµå¼ä¼ è¾“ï¼‰"""
    try:
        query = request.query.strip()
        if not query:
            raise HTTPException(status_code=400, detail='é—®é¢˜ä¸èƒ½ä¸ºç©º')

        session_id = request.session_id

        # å¦‚æœæ²¡æœ‰ session_idï¼Œåˆ›å»ºæ–°ä¼šè¯
        if not session_id:
            session_id = str(uuid.uuid4())
            sessions[session_id] = []
        elif session_id not in sessions:
            sessions[session_id] = []

        # è·å–å†å²è®°å½•
        chat_history = sessions[session_id]

        rag = get_rag()

        async def event_generator():
            # å‘é€ session_id
            yield f"data: {json.dumps({'type': 'session', 'session_id': session_id}, ensure_ascii=False)}\n\n"

            # æµå¼ç”Ÿæˆç­”æ¡ˆ
            full_answer = ""
            for chunk_data in rag.generate_answer_stream(
                    query,
                    chat_history=chat_history,
                    n_results=request.n_results,
                    similarity_threshold=request.similarity_threshold
            ):
                yield f"data: {chunk_data}\n\n"

                # æ”¶é›†å®Œæ•´ç­”æ¡ˆ
                chunk_obj = json.loads(chunk_data)
                if chunk_obj['type'] == 'content':
                    full_answer += chunk_obj['content']

                await asyncio.sleep(0.01)  # å°å»¶è¿Ÿï¼Œè®©æµæ›´å¹³æ»‘

            # æ›´æ–°ä¼šè¯å†å²
            sessions[session_id].append({
                'role': 'user',
                'parts': [{'text': query}]
            })
            sessions[session_id].append({
                'role': 'model',
                'parts': [{'text': full_answer}]
            })

        return StreamingResponse(
            event_generator(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no"
            }
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/search")
async def search_documents(request: QueryRequest):
    """æœç´¢ç›¸å…³æ–‡æ¡£ï¼ˆéæµå¼ï¼‰"""
    try:
        query = request.query.strip()
        if not query:
            raise HTTPException(status_code=400, detail='æŸ¥è¯¢å†…å®¹ä¸èƒ½ä¸ºç©º')

        rag = get_rag()
        search_results = rag.search(
            query,
            request.n_results,
            request.similarity_threshold
        )

        results = []
        if search_results['documents'][0]:
            for i, (doc, metadata, distance) in enumerate(zip(
                    search_results['documents'][0],
                    search_results['metadatas'][0],
                    search_results['distances'][0]
            )):
                keyword_score = search_results.get('keyword_scores', [[]])[0][
                    i] if 'keyword_scores' in search_results else 0

                results.append({
                    'document': doc,
                    'source': metadata.get('source', ''),
                    'section_path': metadata.get('section_path', ''),
                    'keywords': metadata.get('keywords', ''),
                    'chunk_id': metadata.get('chunk_id', ''),
                    'semantic_similarity': round((1 - distance) * 100, 2),
                    'keyword_score': round(keyword_score * 100, 2)
                })

        return {
            'success': True,
            'query': query,
            'results': results,
            'count': len(results)
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/reload")
async def reload_documents(request: ReloadRequest):
    """é‡æ–°åŠ è½½æ–‡æ¡£"""
    try:
        rag = get_rag()
        rag.load_documents_from_folder(
            folder_path="./docs",
            force_reload=request.force_reload
        )

        return {
            'success': True,
            'message': 'æ–‡æ¡£é‡æ–°åŠ è½½å®Œæˆ',
            'total_chunks': rag.collection.count()
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


app.mount("/", StaticFiles(directory="web", html=True), name="web")

# ä¹Ÿå¯ä»¥ç”¨ä»¥ä¸‹æ–¹å¼ï¼Œæ˜¾å¼åœ°ä¸ºæ ¹è·¯å¾„æä¾›index.html
# @app.get("/")
# async def read_index():
#    return FileResponse('web/index.html')


def start_fastapi(host='0.0.0.0', port=8000):
    """å¯åŠ¨ FastAPI æœåŠ¡"""
    import uvicorn

    print("\n" + "=" * 70)
    print("ğŸš€ å¯åŠ¨ FastAPI æœåŠ¡...")
    print("=" * 70)

    print("\nğŸ“¦ é¢„åŠ è½½ RAG ç³»ç»Ÿ...")
    get_rag()

    print("\nâœ… RAG ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    print(f"ğŸŒ Web UI è®¿é—®åœ°å€: http://{host}:{port}")
    print(f"ğŸ“š API æ–‡æ¡£ (Swagger): http://{host}:{port}/docs")
    print("\nğŸ“¡ å¯ç”¨çš„ API ç«¯ç‚¹:")
    print(f"   â€¢ GET  /api/health              - å¥åº·æ£€æŸ¥")
    print(f"   â€¢ GET  /api/info                - ç³»ç»Ÿä¿¡æ¯")
    print(f"   â€¢ POST /api/session/create      - åˆ›å»ºä¼šè¯")
    print(f"   â€¢ POST /api/session/clear       - æ¸…ç©ºä¼šè¯")
    print(f"   â€¢ GET  /api/session/:id/history - è·å–å†å²")
    print(f"   â€¢ POST /api/ask/stream          - RAG é—®ç­”ï¼ˆæµå¼ï¼‰")
    print(f"   â€¢ POST /api/search              - æœç´¢æ–‡æ¡£")
    print(f"   â€¢ POST /api/reload              - é‡æ–°åŠ è½½æ–‡æ¡£")
    print("=" * 70 + "\n")

    uvicorn.run(app, host=host, port=port)


if __name__ == "__main__":
    # ç§»é™¤åŸæœ‰çš„main()å‡½æ•°è°ƒç”¨å’Œå‘½ä»¤è¡Œå‚æ•°åˆ¤æ–­
    # ç›´æ¥å¯åŠ¨FastAPIæœåŠ¡
    start_fastapi(host='0.0.0.0', port=8000)